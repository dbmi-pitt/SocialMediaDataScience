{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")\n",
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. so this is a bit opaque.  is sci-kit any clearer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the scikit spacy combination here ---\n",
    "\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \n",
    "        \"lol @twitterdude that is gr8\", \"twitter &amp; reddit are fun.\", \n",
    "        \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \n",
    "        \"Rockets launch from Earth and go to other planets.\", \"twitter social media &gt; &lt;\", \n",
    "        \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 23, 'space': 39, 'is': 18, 'great': 13, 'planets': 31, 'are': 3, 'cool': 5, 'am': 0, 'glad': 10, 'they': 43, 'exist': 7, 'in': 17, 'lol': 22, 'twitterdude': 46, 'that': 41, 'gr8': 12, 'twitter': 45, 'amp': 1, 'reddit': 33, 'fun': 9, 'mars': 25, 'planet': 30, 'it': 19, 'red': 32, 'microsoft': 27, 'skip': 35, 'windows': 47, 'rockets': 34, 'launch': 20, 'from': 8, 'earth': 6, 'and': 2, 'go': 11, 'to': 44, 'other': 29, 'social': 36, 'media': 26, 'gt': 15, 'lt': 24, 'someguy': 38, 'somegirl': 37, 'hashtag': 16, 'orbiting': 28, 'the': 42, 'sun': 40, 'little': 21, 'blue': 4, 'green': 14}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'amp',\n",
       " 'and',\n",
       " 'are',\n",
       " 'blue',\n",
       " 'cool',\n",
       " 'earth',\n",
       " 'exist',\n",
       " 'from',\n",
       " 'fun',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'green',\n",
       " 'gt',\n",
       " 'hashtag',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'launch',\n",
       " 'little',\n",
       " 'lol',\n",
       " 'love',\n",
       " 'lt',\n",
       " 'mars',\n",
       " 'media',\n",
       " 'microsoft',\n",
       " 'orbiting',\n",
       " 'other',\n",
       " 'planet',\n",
       " 'planets',\n",
       " 'red',\n",
       " 'reddit',\n",
       " 'rockets',\n",
       " 'skip',\n",
       " 'social',\n",
       " 'somegirl',\n",
       " 'someguy',\n",
       " 'space',\n",
       " 'sun',\n",
       " 'that',\n",
       " 'the',\n",
       " 'they',\n",
       " 'to',\n",
       " 'twitter',\n",
       " 'twitterdude',\n",
       " 'windows']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=vectorizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 23, 'space': 39, 'is': 18, 'great': 13, 'planets': 31, 'are': 3, 'cool': 5, 'am': 0, 'glad': 10, 'they': 43, 'exist': 7, 'in': 17, 'lol': 22, 'twitterdude': 46, 'that': 41, 'gr8': 12, 'twitter': 45, 'amp': 1, 'reddit': 33, 'fun': 9, 'mars': 25, 'planet': 30, 'it': 19, 'red': 32, 'microsoft': 27, 'skip': 35, 'windows': 47, 'rockets': 34, 'launch': 20, 'from': 8, 'earth': 6, 'and': 2, 'go': 11, 'to': 44, 'other': 29, 'social': 36, 'media': 26, 'gt': 15, 'lt': 24, 'someguy': 38, 'somegirl': 37, 'hashtag': 16, 'orbiting': 28, 'the': 42, 'sun': 40, 'little': 21, 'blue': 4, 'green': 14}\n",
      "[2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 1.78845736 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(train)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43323568 0.         0.         0.         0.\n",
      "  0.28646791 0.         0.         0.         0.         0.43323568\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.73657982 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.34989927 0.         0.         0.29744635 0.         0.34989927\n",
      "  0.         0.34989927 0.         0.         0.34989927 0.\n",
      "  0.         0.         0.         0.         0.         0.34989927\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29744635 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29744635 0.         0.\n",
      "  0.         0.34989927 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47472745 0.         0.         0.         0.         0.\n",
      "  0.31390347 0.         0.         0.         0.47472745 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.47472745\n",
      "  0.         0.         0.         0.         0.47472745 0.        ]\n",
      " [0.         0.48360622 0.         0.41110947 0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.3596722  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.56536198 0.42750858 0.         0.         0.         0.\n",
      "  0.         0.42750858 0.         0.         0.         0.\n",
      "  0.36342135 0.         0.42750858 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027]\n",
      " [0.         0.         0.33859118 0.         0.         0.\n",
      "  0.33859118 0.         0.33859118 0.         0.         0.33859118\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33859118\n",
      "  0.         0.28783344 0.         0.         0.33859118 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.46864588 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.46864588 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34854576 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.53051081 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.53051081 0.53051081 0.         0.         0.\n",
      "  0.         0.         0.         0.39455653 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37372071 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37372071 0.         0.         0.\n",
      "  0.2471149  0.         0.         0.37372071 0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.31769674 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.37372071 0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(train)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. now -- how can I train on this? \n",
    "and how can I pass in my spacy tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plan\n",
    "\n",
    "1. Go back to https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "1.1 create NLP as per part 3 with hashtag tokenizer.\n",
    "\n",
    "1.2. wrap that in a tokenizer that calls the spacy NLP parser and tokenizes with stops, etc.\n",
    "\n",
    "1.3 use that tokenizer in a tf-df\n",
    "\n",
    "1.4 follow that up with classifier. \n",
    "\n",
    "1.5 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = True\n",
    "    while merged_hashtag == True:\n",
    "        merged_hashtag = False\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                try:\n",
    "                    nbor = token.nbor()\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "    return doc\n",
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(sample):\n",
    "    tokens = nlp(sample)\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= getTwitterNLP()\n",
    "\n",
    "def filterTweetTokens2(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    for ent in tokens.ents:\n",
    "        stripped =ent.text.strip()\n",
    "        if len(stripped) >0:\n",
    "            filtered.append(ent.text.strip())\n",
    "    return filtered\n",
    "\n",
    "def tokenizeText2(text):\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens2(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - set up tokenizer in a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer(tokenizer=tokenizeText)\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=tokenizeText2)\n",
    "clf = LinearSVC()\n",
    "clf2 = LinearSVC()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "pipe2= Pipeline([('vectorizer', vectorizer2), ('clf', clf2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 now I would do vectorizer. fit - need good input and labels.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x10ce62978>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x10ce62a90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ok. to do this I think the following needs to happen\n",
    "1. flatten the lists into pairs of text and category\n",
    "2. shuffle and pick 0.8 of each contaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenTweets(tweets):\n",
    "    flat=[]\n",
    "    for i in tweets.getIds():\n",
    "        text = tweets.getText(i)\n",
    "        cat = tweets.getSearchTerm(i) \n",
    "        pair =(text,cat)\n",
    "        flat.append(pair)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainSplit(pairs,splitFactor=0.8):\n",
    "    random.shuffle(pairs)\n",
    "    split=int(len(pairs)*splitFactor)\n",
    "    train=pairs[:split]\n",
    "    test =pairs[split:]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrain(smoking,vaping):\n",
    "    fsmoke=flattenTweets(smoking)\n",
    "    fvape=flattenTweets(vaping)\n",
    "    strain,stest=getTestTrainSplit(fsmoke)\n",
    "    vtrain,vtest=getTestTrainSplit(fvape)\n",
    "    train=strain+vtrain\n",
    "    test=stest+vtest\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrain2(smoking,vaping):\n",
    "    fsmoke=flattenTweets(smoking)\n",
    "    fvape=flattenTweets(vaping)\n",
    "    tweets = fsmoke+fvape\n",
    "    train,test=getTestTrainSplit(tweets)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok. split established, but something is not right. need to ensure right split. \n",
    "# an then will need to split train and test into cateogires and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=getTestTrain(smoking,vaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTweets,trainCats=zip(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Jodymurph6 You back smoking that funny stuff lad'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainCats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smoking',\n",
       " 'smoking',\n",
       " 'vaping',\n",
       " 'vaping',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'vaping',\n",
       " 'smoking')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCats[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to run through the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(trainTweets,trainCats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ok. did that do anything? Let's get the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweets,testCats=zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. 97.5% accurate classification. can we look at the mismatches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip up input, category, prediction.  Look at first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " (\"Vaping: It's just as unhealthy as smoking and it makes you look like a dork.\",\n",
       "  'vaping',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since thenü§¶üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('To celebrate #VApril we‚Äôre giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " (\"@TradFemdom Nigga you'd start smoking why the fuck\", 'smoking', 'smoking'),\n",
       " ('First Squonk Kit by ASPIRE - Feedlink Squonk Mod is coming! 7ml e juice capacity &amp; 5 protections Get it&gt;&gt; https://t.co/8xG18deX65\\n#aspiresquonk #aspirefeeklink #aspire #squonk #revvotank #vaping #cloudchasers #vapelife #vapor https://t.co/gzOTOjKygT',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Hey, #Twitter #fam! Check out this #exclusive #sale just for #YOU! #vaoe #vaping #vapeon #vapelife #vapenation #venumvapur https://t.co/bGxfvTsfka',\n",
       "  'vaping',\n",
       "  'vaping')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out those that don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vaping: It's just as unhealthy as smoking and it makes you look like a dork., vaping, smoking\n",
      "Brave man apologising well done ant https://t.co/KmS6eqt91x, vaping, smoking\n"
     ]
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "for (sample,label,pred) in res:\n",
    "    if label != pred:\n",
    "        print(sample+\", \"+label+\", \"+pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. must put this into the text to challenge people to make the record better.  Then, challenge them to annotate and filter.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simlar for pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.fit(trainTweets,trainCats)\n",
    "preds = pipe2.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " (\"Vaping: It's just as unhealthy as smoking and it makes you look like a dork.\",\n",
       "  'vaping',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since thenü§¶üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('To celebrate #VApril we‚Äôre giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " (\"@TradFemdom Nigga you'd start smoking why the fuck\", 'smoking', 'smoking'),\n",
       " ('First Squonk Kit by ASPIRE - Feedlink Squonk Mod is coming! 7ml e juice capacity &amp; 5 protections Get it&gt;&gt; https://t.co/8xG18deX65\\n#aspiresquonk #aspirefeeklink #aspire #squonk #revvotank #vaping #cloudchasers #vapelife #vapor https://t.co/gzOTOjKygT',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Hey, #Twitter #fam! Check out this #exclusive #sale just for #YOU! #vaoe #vaping #vapeon #vapelife #vapenation #venumvapur https://t.co/bGxfvTsfka',\n",
       "  'vaping',\n",
       "  'vaping')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trying alternative test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2,test2=getTestTrain2(smoking,vaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTweets2,trainCats2=zip(*train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(trainTweets2,trainCats2)\n",
    "testTweets2,testCats2=zip(*test2)\n",
    "preds2 = pipe.predict(testTweets2)\n",
    "print(\"accuracy:\", accuracy_score(testCats2, preds2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('vaping', 'smoking', 'smoking', 'vaping', 'vaping', 'smoking', 'smoking', 'vaping', 'smoking', 'smoking', 'smoking', 'smoking', 'smoking', 'vaping', 'smoking', 'smoking', 'smoking', 'vaping', 'vaping', 'smoking', 'smoking', 'smoking', 'smoking', 'vaping', 'smoking', 'smoking', 'smoking', 'vaping', 'vaping', 'vaping', 'vaping', 'smoking', 'smoking', 'vaping', 'vaping', 'smoking', 'vaping', 'smoking', 'vaping', 'vaping')\n",
      "['vaping' 'smoking' 'smoking' 'vaping' 'vaping' 'smoking' 'smoking'\n",
      " 'vaping' 'smoking' 'smoking' 'smoking' 'smoking' 'smoking' 'vaping'\n",
      " 'smoking' 'smoking' 'smoking' 'vaping' 'vaping' 'smoking' 'vaping'\n",
      " 'smoking' 'smoking' 'vaping' 'smoking' 'smoking' 'smoking' 'vaping'\n",
      " 'vaping' 'vaping' 'vaping' 'smoking' 'smoking' 'vaping' 'vaping'\n",
      " 'smoking' 'vaping' 'smoking' 'vaping' 'vaping']\n"
     ]
    }
   ],
   "source": [
    "print(testCats2)\n",
    "print(preds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeing kids smoking in school uniform makes me so angry!, smoking, vaping\n"
     ]
    }
   ],
   "source": [
    "res=zip(testTweets2,testCats2,preds2)\n",
    "for (sample,label,pred) in res:\n",
    "    if label != pred:\n",
    "        print(sample+\", \"+label+\", \"+pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=pipe.decision_function(testTweets2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.44442886, -1.72235189, -0.83973348,  0.97841829,  1.25158901,\n",
       "       -0.39922216, -0.98729157,  0.94179493, -0.76913244, -1.24619437,\n",
       "       -0.60125976, -0.99272123, -0.98729157,  1.23612057, -2.04450307,\n",
       "       -0.79337801, -0.92409188,  0.73309948,  1.05301543, -0.82566872,\n",
       "        0.08063856, -0.61427309, -0.98411071,  0.99040477, -0.99272123,\n",
       "       -0.83150767, -0.58095218,  0.86652445,  0.76453999,  0.39503961,\n",
       "        0.43271291, -0.99272123, -0.99272123,  0.83333361,  1.07314745,\n",
       "       -0.98729157,  2.37131205, -0.37666079,  0.38231714,  0.59321341])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCats=[]\n",
    "for i in range(len(testCats2)):\n",
    "    if testCats2[i] =='vaping':\n",
    "        numCats.append(1)\n",
    "    else:\n",
    "        numCats.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_precision_score(numCats,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['smoking', 'smoking', 'smoking', 'smoking', 'smoking', 'smoking',\n",
       "       'vaping', 'smoking', 'vaping', 'vaping', 'vaping', 'smoking',\n",
       "       'smoking', 'vaping', 'vaping', 'smoking', 'vaping', 'smoking',\n",
       "       'vaping', 'smoking', 'vaping', 'vaping', 'smoking', 'vaping',\n",
       "       'smoking', 'smoking', 'vaping', 'smoking', 'vaping', 'smoking',\n",
       "       'smoking', 'vaping', 'smoking', 'vaping', 'smoking', 'vaping',\n",
       "       'smoking', 'vaping', 'smoking', 'vaping'], dtype='<U7')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPreds=[]\n",
    "for i in range(len(preds2)):\n",
    "    if preds2[i] =='vaping':\n",
    "        numPreds.append(1)\n",
    "    else:\n",
    "        numPreds.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(numCats,numPreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1]\n",
      "[1, -1, -1, 1, 1, -1, -1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, -1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, 1, 1, -1, 1, -1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numCats)\n",
    "print(numPreds)\n",
    "errs=[]\n",
    "for i in range(len(numCats)):\n",
    "    if numCats[i]!=numPreds[i]:\n",
    "        errs.append((i,numCats[i],numPreds[i]))\n",
    "len(errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what gives here? somehow claimed good accuracy and precision, but shit recalll? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoking smoking\n",
      "smoking smoking\n",
      "vaping smoking\n",
      "smoking smoking\n",
      "smoking smoking\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "vaping vaping\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping smoking\n",
      "vaping vaping\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n",
      "smoking smoking\n",
      "vaping vaping\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(testTweets)):\n",
    "    print(testCats[i]+ \" \"+preds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "simtexts=[\"The team made their way thorugh the season.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer=CountVectorizer()\n",
    "simvectorizer.fit(simtexts)\n",
    "simvec =simvectorizer.transform(simtexts)\n",
    "print(simvec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 3, 'team': 2, 'made': 0, 'their': 4, 'way': 6, 'thorugh': 5, 'season': 1}\n"
     ]
    }
   ],
   "source": [
    "print(simvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp\n",
    "\n",
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val\n",
    "\n",
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    nlp=getTwitterNLP()\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens(tokens)\n",
    "\n",
    "simvectorizer2=CountVectorizer(tokenizer=tokenizeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team': 1, 'way': 2, 'season': 0}\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer2.fit(simtexts)\n",
    "simvec2 =simvectorizer2.transform(simtexts)\n",
    "print(simvectorizer2.vocabulary_)\n",
    "print(simvec2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "simvectorizer2.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'team': 1, 'way': 2, 'season': 0}\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer3=CountVectorizer(tokenizer=tokenizeText,analyzer='word')\n",
    "simvectorizer3.fit(simtexts)\n",
    "simvec3=simvectorizer3.transform(simtexts)\n",
    "print(simvectorizer3.vocabulary_)\n",
    "print(simvec3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The team made their way thorugh the season.']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simtexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['team', 'way', 'season']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeText(simtexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm. now it is DOING IT  right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2=[\"Welcome to Eternity...Smoking or non smoking ? https://t.co/W9nAU7GEaQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 8, 'to': 6, 'eternity': 1, 'smoking': 5, 'or': 4, 'non': 3, 'https': 2, 'co': 0, 'w9nau7geaq': 7}\n",
      "[[1 1 1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer=CountVectorizer()\n",
    "simvectorizer.fit(texts2)\n",
    "simvec =simvectorizer.transform(texts2)\n",
    "print(simvectorizer.vocabulary_)\n",
    "print(simvec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 2, 'eternity': 0, 'smoking': 1}\n",
      "[[1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "def preProcess(x):\n",
    "    return x\n",
    "simvectorizer3=CountVectorizer(tokenizer=tokenizeText,preprocessor=preProcess,analyzer='word')\n",
    "simvectorizer3.fit(texts2)\n",
    "simvec3=simvectorizer3.transform(texts2)\n",
    "print(simvectorizer3.vocabulary_)\n",
    "print(simvec3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks=tokenizeText(texts2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', 'eternity', 'smoking', 'smoking']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ah. ah. somehow it's processing stuff tht pulls out 'welcome' why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to turn off the preprocessor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
