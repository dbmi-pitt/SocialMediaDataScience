{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")\n",
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. so this is a bit opaque.  is sci-kit any clearer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the scikit spacy combination here ---\n",
    "\n",
    "https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [\"I love space. Space is great.\", \"Planets are cool. I am glad they exist in space\", \n",
    "        \"lol @twitterdude that is gr8\", \"twitter &amp; reddit are fun.\", \n",
    "        \"Mars is a planet. It is red.\", \"@Microsoft: y u skip windows 9?\", \n",
    "        \"Rockets launch from Earth and go to other planets.\", \"twitter social media &gt; &lt;\", \n",
    "        \"@someguy @somegirl @twitter #hashtag\", \"Orbiting the sun is a little blue-green planet.\"]\n",
    "labelsTrain = [\"space\", \"space\", \"twitter\", \"twitter\", \"space\", \"twitter\", \"space\", \"twitter\", \"twitter\", \"space\"]\n",
    "\n",
    "test = [\"i h8 riting comprehensibly #skoolsux\", \"planets and stars and rockets and stuff\"]\n",
    "labelsTest = [\"twitter\", \"space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 32, 'orbiting': 28, 'blue': 4, 'sun': 40, 'in': 17, 'glad': 10, 'someguy': 38, 'exist': 7, 'earth': 6, 'to': 44, 'is': 18, 'great': 13, 'the': 42, 'other': 29, 'amp': 1, 'reddit': 33, 'social': 36, 'that': 41, 'microsoft': 27, 'it': 19, 'hashtag': 16, 'planets': 31, 'cool': 5, 'from': 8, 'launch': 20, 'planet': 30, 'are': 3, 'media': 26, 'fun': 9, 'somegirl': 37, 'green': 14, 'gt': 15, 'am': 0, 'space': 39, 'lol': 22, 'go': 11, 'little': 21, 'they': 43, 'rockets': 34, 'gr8': 12, 'twitterdude': 46, 'twitter': 45, 'love': 23, 'skip': 35, 'windows': 47, 'lt': 24, 'mars': 25, 'and': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['am',\n",
       " 'amp',\n",
       " 'and',\n",
       " 'are',\n",
       " 'blue',\n",
       " 'cool',\n",
       " 'earth',\n",
       " 'exist',\n",
       " 'from',\n",
       " 'fun',\n",
       " 'glad',\n",
       " 'go',\n",
       " 'gr8',\n",
       " 'great',\n",
       " 'green',\n",
       " 'gt',\n",
       " 'hashtag',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'launch',\n",
       " 'little',\n",
       " 'lol',\n",
       " 'love',\n",
       " 'lt',\n",
       " 'mars',\n",
       " 'media',\n",
       " 'microsoft',\n",
       " 'orbiting',\n",
       " 'other',\n",
       " 'planet',\n",
       " 'planets',\n",
       " 'red',\n",
       " 'reddit',\n",
       " 'rockets',\n",
       " 'skip',\n",
       " 'social',\n",
       " 'somegirl',\n",
       " 'someguy',\n",
       " 'space',\n",
       " 'sun',\n",
       " 'that',\n",
       " 'the',\n",
       " 'they',\n",
       " 'to',\n",
       " 'twitter',\n",
       " 'twitterdude',\n",
       " 'windows']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_.get(\"windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=vectorizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      "  1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 1 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'red': 32, 'orbiting': 28, 'blue': 4, 'sun': 40, 'in': 17, 'glad': 10, 'someguy': 38, 'exist': 7, 'earth': 6, 'to': 44, 'is': 18, 'great': 13, 'the': 42, 'other': 29, 'amp': 1, 'reddit': 33, 'social': 36, 'that': 41, 'microsoft': 27, 'it': 19, 'hashtag': 16, 'planets': 31, 'cool': 5, 'from': 8, 'launch': 20, 'planet': 30, 'are': 3, 'media': 26, 'fun': 9, 'somegirl': 37, 'green': 14, 'gt': 15, 'am': 0, 'space': 39, 'lol': 22, 'go': 11, 'little': 21, 'they': 43, 'rockets': 34, 'gr8': 12, 'twitterdude': 46, 'twitter': 45, 'love': 23, 'skip': 35, 'windows': 47, 'lt': 24, 'mars': 25, 'and': 2}\n",
      "[2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 1.78845736 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.29928298 2.29928298 2.70474809 2.70474809 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.29928298 2.70474809 2.70474809\n",
      " 2.70474809 2.70474809 2.70474809 2.01160091 2.70474809 2.70474809]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(train)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 48)\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.43323568 0.         0.         0.         0.\n",
      "  0.28646791 0.         0.         0.         0.         0.43323568\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.73657982 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.34989927 0.         0.         0.29744635 0.         0.34989927\n",
      "  0.         0.34989927 0.         0.         0.34989927 0.\n",
      "  0.         0.         0.         0.         0.         0.34989927\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29744635 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29744635 0.         0.\n",
      "  0.         0.34989927 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.47472745 0.         0.         0.         0.         0.\n",
      "  0.31390347 0.         0.         0.         0.47472745 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.47472745\n",
      "  0.         0.         0.         0.         0.47472745 0.        ]\n",
      " [0.         0.48360622 0.         0.41110947 0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.48360622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.3596722  0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.56536198 0.42750858 0.         0.         0.         0.\n",
      "  0.         0.42750858 0.         0.         0.         0.\n",
      "  0.36342135 0.         0.42750858 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027]\n",
      " [0.         0.         0.33859118 0.         0.         0.\n",
      "  0.33859118 0.         0.33859118 0.         0.         0.33859118\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.33859118\n",
      "  0.         0.28783344 0.         0.         0.33859118 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.33859118 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.46864588 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.46864588 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.46864588 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.34854576 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.53051081 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.53051081 0.53051081 0.         0.         0.\n",
      "  0.         0.         0.         0.39455653 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.37372071 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37372071 0.         0.         0.\n",
      "  0.2471149  0.         0.         0.37372071 0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.31769674 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.37372071 0.\n",
      "  0.37372071 0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform(train)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. now -- how can I train on this? \n",
    "and how can I pass in my spacy tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plan\n",
    "\n",
    "1. Go back to https://nicschrading.com/project/Intro-to-NLP-with-spaCy/\n",
    "\n",
    "1.1 create NLP as per part 3 with hashtag tokenizer.\n",
    "\n",
    "1.2. wrap that in a tokenizer that calls the spacy NLP parser and tokenizes with stops, etc.\n",
    "\n",
    "1.3 use that tokenizer in a tf-df\n",
    "\n",
    "1.4 follow that up with classifier. \n",
    "\n",
    "1.5 test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = True\n",
    "    while merged_hashtag == True:\n",
    "        merged_hashtag = False\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                try:\n",
    "                    nbor = token.nbor()\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "                except:\n",
    "                    pass\n",
    "    return doc\n",
    "nlp.add_pipe(hashtag_pipe,first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(sample):\n",
    "    tokens = nlp(sample)\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= getTwitterNLP()\n",
    "\n",
    "def filterTweetTokens2(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    for ent in tokens.ents:\n",
    "        stripped =ent.text.strip()\n",
    "        if len(stripped) >0:\n",
    "            filtered.append(ent.text.strip())\n",
    "    return filtered\n",
    "\n",
    "def tokenizeText2(text):\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens2(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 - set up tokenizer in a vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer= TfidfVectorizer(tokenizer=tokenizeText)\n",
    "vectorizer2 = TfidfVectorizer(tokenizer=tokenizeText2)\n",
    "clf = LinearSVC()\n",
    "clf2 = LinearSVC()\n",
    "pipe = Pipeline([('vectorizer', vectorizer), ('clf', clf)])\n",
    "pipe2= Pipeline([('vectorizer', vectorizer2), ('clf', clf2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4 now I would do vectorizer. fit - need good input and labels.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x10cd584e0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Tweets at 0x140fd25c0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ok. to do this I think the following needs to happen\n",
    "1. flatten the lists into pairs of text and category\n",
    "2. shuffle and pick 0.8 of each contaat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenTweets(tweets):\n",
    "    flat=[]\n",
    "    for i in tweets.getIds():\n",
    "        text = tweets.getText(i)\n",
    "        cat = tweets.getSearchTerm(i) \n",
    "        pair =(text,cat)\n",
    "        flat.append(pair)\n",
    "    return flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrainSplit(pairs,splitFactor=0.8):\n",
    "    random.shuffle(pairs)\n",
    "    split=int(len(pairs)*splitFactor)\n",
    "    train=pairs[:split]\n",
    "    test =pairs[split:]\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTestTrain(smoking,vaping):\n",
    "    fsmoke=flattenTweets(smoking)\n",
    "    fvape=flattenTweets(vaping)\n",
    "    strain,stest=getTestTrainSplit(fsmoke)\n",
    "    vtrain,vtest=getTestTrainSplit(fvape)\n",
    "    train=strain+vtrain\n",
    "    test=stest+vtest\n",
    "    random.shuffle(train)\n",
    "    random.shuffle(test)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok. split established, but something is not right. need to ensure right split. \n",
    "# an then will need to split train and test into cateogires and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=getTestTrain(smoking,vaping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTweets,trainCats=zip(*train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Dunham2Times Stg that‚Äôs some cigarette smoking shit because I‚Äôd never. I always wonder what people‚Äôs parents would think of they seen them cutting a fool üòÇ'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainTweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainCats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('smoking',\n",
       " 'vaping',\n",
       " 'vaping',\n",
       " 'smoking',\n",
       " 'vaping',\n",
       " 'vaping',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'smoking',\n",
       " 'smoking')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainCats[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to run through the pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(trainTweets,trainCats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ok. did that do anything? Let's get the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTweets,testCats=zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipe.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. 97.5% accurate classification. can we look at the mismatches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip up input, category, prediction.  Look at first few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Smoking a BIG DICK if yu think I need yuüòè', 'smoking', 'smoking'),\n",
       " ('What are the V2 Vape Pens https://t.co/vrNJsq9N27 Remember to use our #coupon SOFLA10 during checkout #Vape #Vaping #VapeBlast #RT #eCigs',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('$26.50! SMOK Stick One Plus Starter Kit 2000mah Battery Micro TFV4 Plus Tank https://t.co/j09lhgWtTT #vape #ecig #vaping #vapelife #vapefam #ejuice #ecigbattery .@Cuecig https://t.co/BkPTWYvT4p',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('RT MtBakerVapor Get #MtBakerVapor at https://t.co/cZo4rT0bgU \"NOW AVAILABLE - yamivapor - 8 different flavors, sure to satisfy your taste buds! #vaping #ejuice\\n.\\nGet yours now: https://t.co/QH72YlTdZE https://t.co/i6JGyGy649\" #ejuice #eliquid #vape',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Welcome to Eternity...Smoking or non smoking ? https://t.co/W9nAU7GEaQ',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('To celebrate #VApril we‚Äôre giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter out those that don't match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "for (sample,label,pred) in res:\n",
    "    if label != pred:\n",
    "        print(sample+\", \"+label+\", \"+pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok. must put this into the text to challenge people to make the record better.  Then, challenge them to annotate and filter.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simlar for pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2.fit(trainTweets,trainCats)\n",
    "preds = pipe2.predict(testTweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy:\", accuracy_score(testCats, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Smoking a BIG DICK if yu think I need yuüòè', 'smoking', 'smoking'),\n",
       " ('What are the V2 Vape Pens https://t.co/vrNJsq9N27 Remember to use our #coupon SOFLA10 during checkout #Vape #Vaping #VapeBlast #RT #eCigs',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('$26.50! SMOK Stick One Plus Starter Kit 2000mah Battery Micro TFV4 Plus Tank https://t.co/j09lhgWtTT #vape #ecig #vaping #vapelife #vapefam #ejuice #ecigbattery .@Cuecig https://t.co/BkPTWYvT4p',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('RT MtBakerVapor Get #MtBakerVapor at https://t.co/cZo4rT0bgU \"NOW AVAILABLE - yamivapor - 8 different flavors, sure to satisfy your taste buds! #vaping #ejuice\\n.\\nGet yours now: https://t.co/QH72YlTdZE https://t.co/i6JGyGy649\" #ejuice #eliquid #vape',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Welcome to Eternity...Smoking or non smoking ? https://t.co/W9nAU7GEaQ',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('me: smoking weed hasn‚Äôt affected me at all\\n\\nsomeone: count to 10\\n\\nme: https://t.co/SUoGzARpom',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('To celebrate #VApril we‚Äôre giving one lucky #vaper a personalised Vapor Space Medium Vapor Box to store their #vaping goodies in! To enter to #win just follow + RT. #Giveaway https://t.co/xJ7mhcs6JG',\n",
       "  'vaping',\n",
       "  'vaping'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking'),\n",
       " ('Made a sandwich 10 min ago and been looking for it ever since then\\U0001f926üèæ\\u200d‚ôÇÔ∏è I gotta stop smokingüòÇ https://t.co/NCbNOyvZXe',\n",
       "  'smoking',\n",
       "  'smoking')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=zip(testTweets,testCats,preds)\n",
    "reslist=list(res)\n",
    "reslist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "simtexts=[\"The team made their way thorugh the season.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer=CountVectorizer()\n",
    "simvectorizer.fit(simtexts)\n",
    "simvec =simvectorizer.transform(simtexts)\n",
    "print(simvec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'season': 1, 'thorugh': 5, 'made': 0, 'the': 3, 'their': 4, 'way': 6, 'team': 2}\n"
     ]
    }
   ],
   "source": [
    "print(simvectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import operator\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime\n",
    "from spacy.symbols import ORTH, LEMMA, POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTwitterNLP():\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    for word in nlp.Defaults.stop_words:\n",
    "        lex = nlp.vocab[word]\n",
    "        lex.is_stop = True\n",
    "    \n",
    "    special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "    nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "    nlp.tokenizer.add_special_case(u'E-cigarette', special_case)\n",
    "    vape_case = [{ORTH: u'vape',LEMMA:u'vape',POS: u'NOUN'}]\n",
    "    \n",
    "    vape_spellings =[u'vap',u'vape',u'vaping',u'vapor',u'Vap',u'Vape',u'Vapor',u'Vapour']\n",
    "    for v in vape_spellings:\n",
    "        nlp.tokenizer.add_special_case(v, vape_case)\n",
    "    def hashtag_pipe(doc):\n",
    "        merged_hashtag = True\n",
    "        while merged_hashtag == True:\n",
    "            merged_hashtag = False\n",
    "            for token_index,token in enumerate(doc):\n",
    "                if token.text == '#':\n",
    "                    try:\n",
    "                        nbor = token.nbor()\n",
    "                        start_index = token.idx\n",
    "                        end_index = start_index + len(token.nbor().text) + 1\n",
    "                        if doc.merge(start_index, end_index) is not None:\n",
    "                            merged_hashtag = True\n",
    "                            break\n",
    "                    except:\n",
    "                        pass\n",
    "        return doc\n",
    "    nlp.add_pipe(hashtag_pipe,first=True)\n",
    "    return nlp\n",
    "\n",
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_stop == False:\n",
    "        if tok.is_alpha == True: \n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='PROPN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        elif tok.text[0]=='#' or tok.text[0]=='@':\n",
    "            val = True\n",
    "    if val== True:\n",
    "        stripped =tok.lemma_.lower().strip()\n",
    "        if len(stripped) ==0:\n",
    "            val = False\n",
    "        else:\n",
    "            val = stripped\n",
    "    return val\n",
    "\n",
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for t in tokens:\n",
    "        inc = includeToken(t)\n",
    "        if inc != False:\n",
    "            filtered.append(inc)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    nlp=getTwitterNLP()\n",
    "    tokens=nlp(text)\n",
    "    return filterTweetTokens(tokens)\n",
    "\n",
    "simvectorizer2=CountVectorizer(tokenizer=tokenizeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'season': 0, 'way': 2, 'team': 1}\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer2.fit(simtexts)\n",
    "simvec2 =simvectorizer2.transform(simtexts)\n",
    "print(simvectorizer2.vocabulary_)\n",
    "print(simvec2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "simvectorizer2.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'season': 0, 'way': 2, 'team': 1}\n",
      "[[1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer3=CountVectorizer(tokenizer=tokenizeText,analyzer='word')\n",
    "simvectorizer3.fit(simtexts)\n",
    "simvec3=simvectorizer3.transform(simtexts)\n",
    "print(simvectorizer3.vocabulary_)\n",
    "print(simvec3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The team made their way thorugh the season.']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simtexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['team', 'way', 'season']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizeText(simtexts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm. now it is DOING IT  right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts2=[\"Welcome to Eternity...Smoking or non smoking ? https://t.co/W9nAU7GEaQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 6, 'non': 3, 'smoking': 5, 'https': 2, 'eternity': 1, 'w9nau7geaq': 7, 'or': 4, 'welcome': 8, 'co': 0}\n",
      "[[1 1 1 1 1 2 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "simvectorizer=CountVectorizer()\n",
    "simvectorizer.fit(texts2)\n",
    "simvec =simvectorizer.transform(texts2)\n",
    "print(simvectorizer.vocabulary_)\n",
    "print(simvec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'welcome': 2, 'smoking': 1, 'eternity': 0}\n",
      "[[1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "def preProcess(x):\n",
    "    return x\n",
    "simvectorizer3=CountVectorizer(tokenizer=tokenizeText,preprocessor=preProcess,analyzer='word')\n",
    "simvectorizer3.fit(texts2)\n",
    "simvec3=simvectorizer3.transform(texts2)\n",
    "print(simvectorizer3.vocabulary_)\n",
    "print(simvec3.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks=tokenizeText(texts2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome', 'eternity', 'smoking', 'smoking']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ah. ah. somehow it's processing stuff tht pulls out 'welcome' why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to turn off the preprocessor..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
