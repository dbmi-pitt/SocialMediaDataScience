{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media and Human-Computer Interaction - Part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Use social media posts to explore the appplication of text and natural language processing to see what might be learned from online interactions.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "Required Software\n",
    "* [Python 3](https://www.python.org)\n",
    "* [NumPy](http://www.numpy.org) - for preparing data for plotting\n",
    "* [Matplotlib](https://matplotlib.org) - plots and garphs\n",
    "* [jsonpickle](https://jsonpickle.github.io) for storing tweets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import tweepy\n",
    "import spacy\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This module continues the Social Media Data Science module started in [Part 1](SocialMedia - Part 1.ipynb), covering the natural language processing analysis of our tweet corpus, including \n",
    "\n",
    "  1. Natural Language Processings\n",
    "  2. Construction of classifiers\n",
    "  \n",
    "Our case study will apply these topics to Twitter discussions of smoking and vaping. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "\n",
    "Before we dig in, we must grab a bit of code from [Part 1](SocialMedia - Part 1.ipynb):\n",
    "\n",
    "1. Our Tweets class\n",
    "3. Our twitter API Keys - be sure to copy the keys that you generated when you completed [Part 1](SocialMedia - Part 1.ipynb).\n",
    "4. Configuration of our Twitter connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "    \n",
    "    # NEW -ROUTINE TO GET PROFILE\n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*REDACT FOLLOWING DETAILS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key='D2L4YZ2YrO1PMix7uKUK63b8H'\n",
    "consumer_secret='losRw9T8zb6VT3TEJ9JHmmhAmn1GXKVj30dkiMv9vjhXuiWek9'\n",
    "access_token='15283934-iggs1hiZAPI2o5sfHWMfjumTF7SvytHPjpPRGf3I6'\n",
    "access_secret='bOvqssxS97PGPwXHQZxk83KtAcDyLhRLgdQaokCdVvwFi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Examination of text patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our ultimate goal is to build a classifier capable of distinguishing tweets related to tobacco smoing from other, unrelated tweets. To do this, we will eventually build natural language processing models. However, we will start by doing some basic processing to explore the types of words and language found in the tweets. \n",
    "\n",
    "To do this, we will use the [Spacy](https://spacy.io/) Python NLP package. Spacy provides significant NLP power out-of-the box, with customization facilities offering greater flexibility at various stages of the Pipeline. Details can be found at the  [Spacy web site](https://spacy.io/), and in this [tutorial](https://nicschrading.com/project/Intro-to-NLP-with-spaCy/).\n",
    "\n",
    "However, before we get into the deails, a bit of a roadmap. \n",
    "\n",
    "Natural Language Processing involves a series of operations on an input text, each building off of the previous step to add additional insight and undertanding.  Thus, many NLP packages run as pipeline processors providing modular components at each stage of the process. Separating key steps into discrete packages provides needed modularity, as developers can modify and customize individual components as needed. Spacy, like other NLP tools including [GATE](https://gate.ac.uk/) and [cTAKES](https://ctaes.apache.org)  operate on such a model. Although the specific components of each pipeline vary from system to system (and from tasks to task, the key tasks are rougly similar:\n",
    "\n",
    "1. *Tokenizing*: splitting the text into words, punctuation, and other markers.\n",
    "2. *Part of speech tagging*: Classifying terms as nouns, verbs, adjective, adverbs, ec.\n",
    "3. *Dependency Parsing* or *Chunking*: Defining relationships between tokens (subject and object of sentence) and grouping into noun and veb phrases.\n",
    "4. *Named Entity Recognition*: Mapping words or phrases to standard vocabularies or other common, known values. This step is often key for linking free text to accepted terms for diseases, symptoms, and/or anatomic locations.\n",
    "\n",
    "Each of these steps might be accomplished through rules, machine learning models, or some combination of approaches. After these initial steps are complete, results might be used to identify relationships between items in the text, build classifiers, or otherwise conduct further analysis. We'll get into these topics later.\n",
    "\n",
    "The [Spacy documentation](https://spacy.io/usage/spacy-101) and [cTAKES default pipeline description](https://cwiki.apache.org/confluence/display/CTAKES/Default+Clinical+Pipeline) provide two examples of how these components might be arranged in practice.  For more information on NLP theory and methods, see [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), perhaps the leading NLP textbook.\n",
    "\n",
    "Given this introduction, we can read in our tweets and get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.1 Reading in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of [Part 2](SocialMedia - Part 2.ipynb) you had saved two sets of tweets one for smoking and one for vaping. Let's read  in the vaping twets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaping=Tweets()\n",
    "vaping.readTweets(\"tweets-vaping.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vaping.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the smoking tweets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we will grab a specifc pre-chosen tweet and process it.  \n",
    "\n",
    "This will give us a beginning feel for what [Spacy](https://spacy.io) can do and how we might use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Smoking affects multiple parts of our body. Know more: https://t.co/hwTeRdC9Hf \\n#SwasthaBharat #NHPIndia #mCessation #QuitSmoking https://t.co/x7xHO9G2Cr'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_id='974316984740429824'\n",
    "sample=smoking.getText(tweet_id)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets have usage patterns that are non-standard English - URLs, hashtags, user references (this particularly tweet was not selected accidentally). These patterns create challenges for extracting content - we might want to know that \"#QuitSmoking\" is, in a tweet, a hashtag that should be considered as a complete unit.  \n",
    "\n",
    "We'll see soon how we might do this, but first, to start the NLP process, we can import the Spacy components and create an NLP object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can then parse out the text from the first tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list of tokens. We can print out each token to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see right away that this parsing isn't quite what we would like. Default English parsing treats  `#QuitSmoking`  as two separate tokens - `#` and `QuitSmoking`. To treat this as a hashtag, we will indeed need to revise the parser.\n",
    "\n",
    "Before we do that, we can also look at some of the othe attributes that we might learn form the parser. \n",
    "\n",
    "Chief among this is the *lemma_*: the \"standard\" or \"base\" form, reducing verb forms to their base verb, plurals to appropriate singular nouns, etc.  For example, the 29th token is `affects`, which has `affect` as the lemmatized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].text)\n",
    "print(parsed[2].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Spacy stores many fields as both hashes for efficiency and as text  for readability. You'll want to use the text form for interpreting results, but the hash for computing. They differ only in the use of the trailing underscore - thus `lemma` is the hash while `lemma_` is the human readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17543419487618836897\n",
      "affect\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].lemma)\n",
    "print(parsed[2].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pos_* and *tag_* provide basic and detailed information on the part of speech. Looking at the 2nd token - \"Smoking\", we see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affects\n",
      "VBZ\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(parsed[2].text)\n",
    "print(parsed[2].tag_)\n",
    "print(parsed[2].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'tag_' and 'pos_' are slightly different versions of the part of speech identified for each token. As described in the [Spacy documentation for part-of-speech tags](https://spacy.io/api/annotation#pos-tagging), the tags associated with these two fields come from different sources. 'tag_' uses parts-of-speech from a version of the [Penn Treebank](https://www.seas.upenn.edu/~pdtb/), a well-known corpus of annotated text. 'pos_' uses a simpler set of tags from [A Universal Part-of-Speech Tagset](https://arxiv.org/abs/1104.2086), published by researchers from Google.  \n",
    "\n",
    "The tags for `affects` provide an example of the difference. According to the [Spacy documentation ](https://spacy.io/api/annotation#pos-tagging) `VBZ` from the Penn tag set indicates a 'verb, 3rd person singular present', while 'the 'VERB' result for 'pos_' corresponds to many types of verbs described in more detail in the Penn Treebank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about a part of spech tag, you can use `spacy.explain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verb\n",
      "verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(parsed[2].pos_)\n",
    "print(spacy.explain(parsed[2].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other values returned by the parser might also be of interest:\n",
    "* is_stop is True if the token is a \"stop\" word - a commonly found word that might addd litle or no information.\n",
    "* is_alpha is True if the token is alphanumeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'Smoking', 'affects', 'multiple', 'parts', 'of', 'our', 'body', '.', 'Know', 'more', ':', 'https://t.co/hwTeRdC9Hf', '\\n', '#', 'SwasthaBharat', '#', 'NHPIndia', '#', 'mCessation', '#', 'QuitSmoking', 'https://t.co/x7xHO9G2Cr']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token 1 (\"Smokingg\"), token 4 (\"parts\"), token 12 (\"https://t.co/hwTeRdC9Hf'\"),  and token 14(\"#\") to see a few more tokens in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoking smoking NOUN NN False True\n",
      "parts part NOUN NNS False True\n",
      "https://t.co/hwTeRdC9Hf https://t.co/hwterdc9hf PROPN NNP False False\n",
      "# # PROPN NNP False False\n"
     ]
    }
   ],
   "source": [
    "t1 = parsed[1]\n",
    "t4 = parsed[4]\n",
    "t12= parsed[12]\n",
    "t14 = parsed[14]\n",
    "print (t1.text,t1.lemma_,t1.pos_,t1.tag_,t1.is_stop,t1.is_alpha)\n",
    "print (t4.text,t4.lemma_,t4.pos_,t4.tag_,t4.is_stop,t4.is_alpha)\n",
    "print (t12.text,t12.lemma_,t12.pos_,t12.tag_,t12.is_stop,t12.is_alpha)\n",
    "print (t14.text,t14.lemma_,t14.pos_,t14.tag_,t14.is_stop,t14.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations:\n",
    "* URLS are neither alphabetical  nor stop-words, but they are proper nouns\n",
    "* '#` is a proper noun. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NLP systems will go a bit further than Spacy's lemmatization, using a process called \"stemming\" to reduce words to base forms. With a stemming algorithm, \"scared\" might be reduced to \"scare\" - see this description of [Porter's stemming algorithm](https://tartarus.org/martin/PorterStemmer/) for more detail. \n",
    "\n",
    "Let's turn the code that we used above into a routine, along with a routine to print out token details and try another tweet or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTweetText(tweets):\n",
    "    tweet_id=random.choice(list(tweets.getIds()))\n",
    "    return tweets.getText(tweet_id)\n",
    "\n",
    "def printTokDetails(parsed):\n",
    "    print(\"{:30} {:30} {:7}{:7}{:7}{:7}\".format(\"Token text\",\"Lemma\",\"POS\",\"Tag\",\"Stop?\",\"Alpha?\"))\n",
    "    for tok in parsed:\n",
    "        print(\"{:30} {:30} {:7}{:7}{:7}{:7}\".format(str(tok.text),str(tok.lemma_),str(tok.pos_),str(tok.tag_),str(tok.is_stop),str(tok.is_alpha)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample2=getTweetText(smoking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#UNFAO is scaling up efforts on reducing the amount of #wood used as #fuel for #fish smoking in the #Gambia. With the new #UNFAO Thiaroye Technology stove, #women in the #fishsmoking &amp; drying #industry have improved access to #technology &amp; #livelihood. #ZeroHunger \\n@FAOWestAfrica https://t.co/ifT8KSRo3O'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed2=nlp(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "#                              #                              PROPN  NNP    False  False  \n",
      "UNFAO                          unfao                          PROPN  NNP    False  True   \n",
      "is                             be                             VERB   VBZ    True   True   \n",
      "scaling                        scale                          VERB   VBG    False  True   \n",
      "up                             up                             PART   RP     True   True   \n",
      "efforts                        effort                         NOUN   NNS    False  True   \n",
      "on                             on                             ADP    IN     True   True   \n",
      "reducing                       reduce                         VERB   VBG    False  True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "amount                         amount                         NOUN   NN     True   True   \n",
      "of                             of                             ADP    IN     True   True   \n",
      "#                              #                              SYM    $      False  False  \n",
      "wood                           wood                           NOUN   NN     False  True   \n",
      "used                           use                            VERB   VBN    True   True   \n",
      "as                             as                             ADP    IN     True   True   \n",
      "#                              #                              NOUN   NN     False  False  \n",
      "fuel                           fuel                           NOUN   NN     False  True   \n",
      "for                            for                            ADP    IN     True   True   \n",
      "#                              #                              ADJ    JJ     False  False  \n",
      "fish                           fish                           NOUN   NN     False  True   \n",
      "smoking                        smoking                        NOUN   NN     False  True   \n",
      "in                             in                             ADP    IN     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "#                              #                              SYM    $      False  False  \n",
      "Gambia                         gambia                         PROPN  NNP    False  True   \n",
      ".                              .                              PUNCT  .      False  False  \n",
      "With                           with                           ADP    IN     False  True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "new                            new                            ADJ    JJ     False  True   \n",
      "#                              #                              SYM    $      False  False  \n",
      "UNFAO                          unfao                          PROPN  NNP    False  True   \n",
      "Thiaroye                       thiaroye                       PROPN  NNP    False  True   \n",
      "Technology                     technology                     PROPN  NNP    False  True   \n",
      "stove                          stove                          NOUN   NN     False  True   \n",
      ",                              ,                              PUNCT  ,      False  False  \n",
      "#                              #                              SYM    $      False  False  \n",
      "women                          woman                          NOUN   NNS    False  True   \n",
      "in                             in                             ADP    IN     True   True   \n",
      "the                            the                            DET    DT     True   True   \n",
      "#                              #                              SYM    $      False  False  \n",
      "fishsmoking                    fishsmok                       VERB   VBG    False  True   \n",
      "&                              &                              CCONJ  CC     False  False  \n",
      "amp                            amp                            NOUN   NN     False  True   \n",
      ";                              ;                              PUNCT  :      False  False  \n",
      "drying                         dry                            VERB   VBG    False  True   \n",
      "#                              #                              SYM    $      False  False  \n",
      "industry                       industry                       NOUN   NN     False  True   \n",
      "have                           have                           VERB   VBP    True   True   \n",
      "improved                       improve                        VERB   VBN    False  True   \n",
      "access                         access                         NOUN   NN     False  True   \n",
      "to                             to                             ADP    IN     True   True   \n",
      "#                              #                              NOUN   NN     False  False  \n",
      "technology                     technology                     NOUN   NN     False  True   \n",
      "&                              &                              CCONJ  CC     False  False  \n",
      "amp                            amp                            NOUN   NN     False  True   \n",
      ";                              ;                              PUNCT  :      False  False  \n",
      "#                              #                              NOUN   NN     False  False  \n",
      "livelihood                     livelihood                     NOUN   NN     False  True   \n",
      ".                              .                              PUNCT  .      False  False  \n",
      "#                              #                              PROPN  NNP    False  False  \n",
      "ZeroHunger                     zerohunger                     PROPN  NNP    False  True   \n",
      "\n",
      "                              \n",
      "                              SPACE         False  False  \n",
      "@FAOWestAfrica                 @faowestafrica                 VERB   VBZ    False  False  \n",
      "https://t.co/ifT8KSRo3O        https://t.co/ift8ksro3o        X      ADD    False  False  \n"
     ]
    }
   ],
   "source": [
    "printTokDetails(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might see some interesting pattners arising here.  For example:\n",
    "\n",
    "* We see many different type of speech. Initially, we might want to focus on the nouns alone, as they provide much of the content.  \n",
    "\n",
    "* Look for words like \"is\" or \"was\" - these might all refer to a common lemma term - \"be\", corresponding to the generic form of he verb. Do you see any other incidents of lemma forms that differ from the parsed text?\n",
    "\n",
    "* URLs and icons might be present in tweets. Are they classified as alphanumeric? Should we include them as part of the \"useful\" text from a tweet? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "## EXERCISE 3.1: Filtering tokens\n",
    "\n",
    "Although NLP parsing is often a good start, further filtering is often necessary to focus on data relevant for specific tasks. In this problem, we will review some additional tweets and develop a post-processing routine capable of filtering tweets as necessary for our needs. \n",
    "\n",
    "3.1.1 Using the `getTweetText`, and `printTokDetails` routines above, aong with the spacy `parser` command, examine several tweets to decide which tokens should be included or not.  List criteria for keeeping/removing tokens. Remember to use `spacy.explain()` for any unfamiliar POS or tag entries. Note that your  criteria will not be perfect, and will likely need refinining. Examiine enough tweets to feel confident in your criteria.\n",
    "\n",
    "3.1.2 Write a routine  `includeToken` that will return True if a token matches the criteria that you identified in 3.11, and false otherwise. Assume for now that we are only interested in nouns and verbs, as they might be a good starting point to find information about vaping or smoking. \n",
    "\n",
    "3.1.3 Write a routine `filterTweetTokens` that will filter the parsed tokens from a single tweet, returning a list of the tokens to be included, based on your criteria.\n",
    "\n",
    "3.1.4 Run `filterTweetTokens` on a few tweets. Identify any inaccuracies and explain them. When possible, identify an approach for improving performance, and implement it in a revision version of `filterTweetTokens`.\n",
    "\n",
    "3.1.5. Add these routines to the tweet class, along with some new routines.\n",
    "\n",
    "3.1.5.1 `parseTweet` will parse one of the tweets in the collection, storing the full list of tokens will be stored in a new entry in the dictionary entitled 'tokens'. `parseTweet` will also filter the tweets, storing the resulting list in an entry entitled 'filteredTokens'.\n",
    "\n",
    "*NOTE*: The tweets class might or might not have an NLP object available for any given call to `parseTweet`. You should have the class create an NLP object when it is initialzed. \n",
    "\n",
    "3.1.5.2 `parseTweets` will call `parseTweet` on all of the tweets in a collection.\n",
    "\n",
    "3.1.5.3 `getTokens` will be used to get all of the tokens for a given tweet.\n",
    "\n",
    "3.1.5.4 `getFilteredTokens` will be used to get all of the filtered tokens for a tweet. \n",
    "\n",
    "\n",
    "3.1.6 When you are done, test this new version of the class by reading in and parsing the 'smoking' tweet set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS Cut below here*\n",
    "\n",
    "### 3.1.1 Sample tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a sandwich 10 min ago and been looking for it ever since thenðŸ¤¦ðŸ¾â€â™‚ï¸ I gotta stop smokingðŸ˜‚ https://t.co/NCbNOyvZXe\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "Made                           make                           VERB   VBN    False  True   \n",
      "a                              a                              DET    DT     True   True   \n",
      "sandwich                       sandwich                       NOUN   NN     False  True   \n",
      "10                             10                             NUM    CD     False  False  \n",
      "min                            min                            NOUN   NN     False  True   \n",
      "ago                            ago                            ADV    RB     False  True   \n",
      "and                            and                            CCONJ  CC     True   True   \n",
      "been                           be                             VERB   VBD    True   True   \n",
      "looking                        look                           VERB   VBG    False  True   \n",
      "for                            for                            ADP    IN     True   True   \n",
      "it                             -PRON-                         PRON   PRP    True   True   \n",
      "ever                           ever                           ADV    RB     True   True   \n",
      "since                          since                          ADP    IN     True   True   \n",
      "then                           then                           ADV    RB     True   True   \n",
      "ðŸ¤¦                              ðŸ¤¦                              VERB   VBN    False  False  \n",
      "ðŸ¾â€                             ðŸ¾â€                             NOUN   NN     False  False  \n",
      "â™‚                              â™‚                              PROPN  NNP    False  False  \n",
      "ï¸                              ï¸                              NOUN   NN     False  False  \n",
      "I                              -PRON-                         PRON   PRP    False  True   \n",
      "got                            get                            VERB   VBD    False  True   \n",
      "ta                             to                             PART   TO     False  True   \n",
      "stop                           stop                           VERB   VB     False  True   \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "ðŸ˜‚                              ðŸ˜‚                              NOUN   NNS    False  False  \n",
      "https://t.co/NCbNOyvZXe        https://t.co/ncbnoyvzxe        NOUN   NNS    False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me: smoking weed hasnâ€™t affected me at all\n",
      "\n",
      "someone: count to 10\n",
      "\n",
      "me: https://t.co/SUoGzARpom\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "me                             -PRON-                         PRON   PRP    True   True   \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "weed                           weed                           NOUN   NN     False  True   \n",
      "has                            have                           VERB   VBZ    True   True   \n",
      "nâ€™t                            not                            ADV    RB     False  False  \n",
      "affected                       affect                         VERB   VBN    False  True   \n",
      "me                             -PRON-                         PRON   PRP    True   True   \n",
      "at                             at                             ADP    IN     True   True   \n",
      "all                            all                            DET    DT     True   True   \n",
      "\n",
      "\n",
      "                             \n",
      "\n",
      "                             SPACE  _SP    False  False  \n",
      "someone                        someone                        NOUN   NN     True   True   \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "count                          count                          VERB   VB     False  True   \n",
      "to                             to                             ADP    IN     True   True   \n",
      "10                             10                             NUM    CD     False  False  \n",
      "\n",
      "\n",
      "                             \n",
      "\n",
      "                             SPACE  _SP    False  False  \n",
      "me                             -PRON-                         PRON   PRP    True   True   \n",
      ":                              :                              PUNCT  :      False  False  \n",
      "https://t.co/SUoGzARpom        https://t.co/suogzarpom        NOUN   NN     False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Artscphoto @presstelegram That's hilarious because I catch those punk ass kids fighting &amp; smoking dope every week by my house.\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "@Artscphoto                    @artscphoto                    NOUN   NN     False  False  \n",
      "@presstelegram                 @presstelegram                 NOUN   NN     False  False  \n",
      "That                           that                           DET    DT     False  True   \n",
      "'s                             be                             VERB   VBZ    False  False  \n",
      "hilarious                      hilarious                      ADJ    JJ     False  True   \n",
      "because                        because                        ADP    IN     True   True   \n",
      "I                              -PRON-                         PRON   PRP    False  True   \n",
      "catch                          catch                          VERB   VBP    False  True   \n",
      "those                          those                          DET    DT     True   True   \n",
      "punk                           punk                           NOUN   NN     False  True   \n",
      "ass                            ass                            NOUN   NN     False  True   \n",
      "kids                           kid                            NOUN   NNS    False  True   \n",
      "fighting                       fighting                       NOUN   NN     False  True   \n",
      "&                              &                              CCONJ  CC     False  False  \n",
      "amp                            amp                            NOUN   NN     False  True   \n",
      ";                              ;                              PUNCT  :      False  False  \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "dope                           dope                           NOUN   NN     False  True   \n",
      "every                          every                          DET    DT     True   True   \n",
      "week                           week                           NOUN   NN     False  True   \n",
      "by                             by                             ADP    IN     True   True   \n",
      "my                             -PRON-                         ADJ    PRP$   True   True   \n",
      "house                          house                          NOUN   NN     False  True   \n",
      ".                              .                              PUNCT  .      False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sexy girls droping there pants pictures of girls smoking virginia slims cigarettes https://t.co/RXYgAr8rKZ\n",
      "Token text                     Lemma                          POS    Tag    Stop?  Alpha? \n",
      "sexy                           sexy                           ADJ    JJ     False  True   \n",
      "girls                          girl                           NOUN   NNS    False  True   \n",
      "droping                        drop                           VERB   VBG    False  True   \n",
      "there                          there                          ADV    RB     True   True   \n",
      "pants                          pant                           NOUN   NNS    False  True   \n",
      "pictures                       picture                        NOUN   NNS    False  True   \n",
      "of                             of                             ADP    IN     True   True   \n",
      "girls                          girl                           NOUN   NNS    False  True   \n",
      "smoking                        smoke                          VERB   VBG    False  True   \n",
      "virginia                       virginia                       NOUN   NN     False  True   \n",
      "slims                          slim                           NOUN   NNS    False  True   \n",
      "cigarettes                     cigarette                      NOUN   NNS    False  True   \n",
      "https://t.co/RXYgAr8rKZ        https://t.co/rxygar8rkz        ADJ    JJ     False  False  \n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)\n",
    "printTokDetails(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria: \n",
    "    \n",
    "* Alpha is true, and \n",
    "* Stop is false, and \n",
    "* text is not \"RT\"\n",
    "* Tag is NN, Tag is NNP, or POS is VERB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2  `includeToken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our routine will accept a token only if it meets the criteria given above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def includeToken(tok):\n",
    "    val =False\n",
    "    if tok.is_alpha == True and tok.is_stop == False:\n",
    "        if tok.text =='RT':\n",
    "            val = False\n",
    "        elif tok.pos_=='NOUN' or tok.pos_=='VERB':\n",
    "            val = True\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love smoking weed in beautiful ass places, looking at beautiful ass things.\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "parsed=nlp(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[0])\n",
    "includeToken(parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[1])\n",
    "includeToken(parsed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smoking\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(parsed[2])\n",
    "includeToken(parsed[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I False\n",
      "love True\n",
      "smoking True\n",
      "weed True\n",
      "in False\n",
      "beautiful False\n",
      "ass True\n",
      "places True\n",
      ", False\n",
      "looking True\n",
      "at False\n",
      "beautiful False\n",
      "ass True\n",
      "things True\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "for tok in parsed:\n",
    "    print(tok,includeToken(tok))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks ok. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Write a routine `filterTweeTokens` that will parse a single tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filterTweetTokens(tokens):\n",
    "    filtered=[]\n",
    "    for tok in tokens:\n",
    "        if includeToken(tok) == True:\n",
    "            filtered.append(tok)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love smoking weed in beautiful ass places, looking at beautiful ass things.\n",
      "love\n",
      "smoking\n",
      "weed\n",
      "ass\n",
      "places\n",
      "looking\n",
      "ass\n",
      "things\n"
     ]
    }
   ],
   "source": [
    "f= filterTweetTokens(parsed)\n",
    "print(sample)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Run `filterTweetTokens` on a few tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cole sprouse smoking appreciation tweet https://t.co/oJdzn6j9lb\n",
      "cole\n",
      "sprouse\n",
      "smoking\n",
      "appreciation\n",
      "tweet\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Zenith #Pilot Type 20 Extra Special #Cohiba-#Maduro 5 Edition watch - A smoking start to 2018 for Zenith/Cohiba partnership with Pilot Type 20 special editions \n",
      "@ZenithWatches #Type20\n",
      "https://t.co/n1CnmMTI2Y https://t.co/H82cdAb7MY\n",
      "watch\n",
      "smoking\n",
      "start\n",
      "partnership\n",
      "editions\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "really tryna stop smoking ðŸ¤¦ðŸ»â€â™€ï¸\n",
      "tryna\n",
      "stop\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made a sandwich 10 min ago and been looking for it ever since thenðŸ¤¦ðŸ¾â€â™‚ï¸ I gotta stop smokingðŸ˜‚ https://t.co/NCbNOyvZXe\n",
      "Made\n",
      "sandwich\n",
      "min\n",
      "looking\n",
      "got\n",
      "stop\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Artscphoto @presstelegram That's hilarious because I catch those punk ass kids fighting &amp; smoking dope every week by my house.\n",
      "catch\n",
      "punk\n",
      "ass\n",
      "kids\n",
      "fighting\n",
      "amp\n",
      "smoking\n",
      "dope\n",
      "week\n",
      "house\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¹€à¸ˆà¹‹à¸‡!ðŸ‘\n",
      "\n",
      "Cool!\n",
      "Sweet! (slang)\n",
      "Great!\n",
      "Beautiful!\n",
      "Awesome!\n",
      "Excellent!\n",
      "That's smoking!\n",
      "That's fab!\n",
      "\n",
      "#à¸šà¸¸à¸žà¹€à¸žà¸ªà¸±à¸™à¸™à¸´à¸§à¸²à¸ª https://t.co/hVqtVvoBp4\n",
      "smoking\n"
     ]
    }
   ],
   "source": [
    "sample=getTweetText(smoking)\n",
    "print(sample)\n",
    "parsed=nlp(sample)\n",
    "f= filterTweetTokens(parsed)\n",
    "for tok in f:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.5  Adding to the Tweets class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        self.nlp = spacy.load('en')\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text\n",
    "                \n",
    "    def addCode(self,id,code):\n",
    "        tweet=self.getTweet(id)\n",
    "        if 'codes' not in tweet:\n",
    "            tweet['codes']=set()\n",
    "        tweet['codes'].add(code)\n",
    "        \n",
    "   \n",
    "    def addCodes(self,id,codes):\n",
    "        for code in codes:\n",
    "            self.addCode(id,code)\n",
    "        \n",
    " \n",
    "    def getCodes(self,id):\n",
    "        tweet=self.getTweet(id)\n",
    "        return tweet['codes']\n",
    "  \n",
    "    def getCodeProfile(self):\n",
    "        summary={}\n",
    "        for id in self.tweets.keys():\n",
    "            tweet=self.getTweet(id)\n",
    "            if 'codes' in tweet:\n",
    "                for code in tweet['codes']:\n",
    "                    if code not in summary:\n",
    "                            summary[code] =0\n",
    "                    summary[code]=summary[code]+1\n",
    "        sortedsummary = sorted(summary.items(),key=operator.itemgetter(0),reverse=True)\n",
    "        return sortedsummary\n",
    "    \n",
    "    # new routine for classifying a token\n",
    "    def includeToken(self,tok):\n",
    "        val =False\n",
    "        if tok.is_alpha == True and tok.is_stop == False:\n",
    "            if tok.text =='RT':\n",
    "                val = False\n",
    "            elif tok.pos_=='NOUN' or tok.pos_=='VERB':\n",
    "                val = True\n",
    "        return val\n",
    "    \n",
    "    # new routine for filtering a list of tokens.\n",
    "    def filterTweetTokens(self,tokens):\n",
    "        filtered=[]\n",
    "        for tok in tokens:\n",
    "            if includeToken(tok) == True:\n",
    "                filtered.append(tok)\n",
    "        return filtered\n",
    "    \n",
    "    def parseTweet(self,id):\n",
    "        text = self.getText(id)\n",
    "        parsed = nlp(text)\n",
    "        self.tweets[id]['tokens']=parsed\n",
    "        filtered= self.filterTweetTokens(parsed)\n",
    "        self.tweets[id]['filteredTokens']=filtered\n",
    "        \n",
    "    def parseTweets(self):\n",
    "        ids=self.getIds()\n",
    "        for id in ids:\n",
    "            self.parseTweet(id)\n",
    "            \n",
    "    def getTokens(self,id):\n",
    "        return self.tweets[id]['tokens']\n",
    "    \n",
    "    def getFilteredTokens(self,id):\n",
    "         return self.tweets[id]['filteredTokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.6 Trying out the new routines for parsing a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoking=Tweets()\n",
    "smoking.readTweets(\"tweets-smoking.json\")\n",
    "smoking.countTweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Made', 'a', 'sandwich', '10', 'min', 'ago', 'and', 'been', 'looking', 'for', 'it', 'ever', 'since', 'then', '\\U0001f926', 'ðŸ¾\\u200d', 'â™‚', 'ï¸', 'I', 'got', 'ta', 'stop', 'smoking', 'ðŸ˜‚', 'https://t.co/NCbNOyvZXe']\n",
      "['Made', 'sandwich', 'min', 'looking', 'got', 'stop', 'smoking']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BTS', 'was', 'nt', 'going', 'to', 'disband', 'what', 'are', 'u', 'smoking', '.', 'Show', 'me', 'where', 'it', 'said', 'they', 'were', 'going', 'to', 'disband', '?', 'only', 'a', 'few', 'of', 'yâ€™', 'all', 'multifandom', 'said', 'u', 'were', 'going', 'to', 'help', 'but', 'we', 'do', 'nâ€™t', 'even', 'know', 'if', 'those', 'really', 'voted', 'https://t.co/CfsHoGnxfJ']\n",
      "['going', 'disband', 'u', 'smoking', 'Show', 'said', 'going', 'disband', 'multifandom', 'said', 'going', 'help', 'know', 'voted']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Our', 'Well', '-', 'Being', 'Advisor', 'Jenny', 'providing', 'some', 'excellent', 'ideas', 'and', ' ', 'quit', 'smoking', 'tips', 'during', 'our', '#', 'NoSmokingDay', '!', 'ðŸš­', '#', 'Pharmacy', '#', 'ThinkPharmacyNI', '#', 'healthpluspharmacy', 'https://t.co/UHn2lSsErQ']\n",
      "['Being', 'providing', 'ideas', 'quit', 'smoking', 'tips', 'ThinkPharmacyNI', 'healthpluspharmacy']\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(smoking.getIds()))\n",
    "smoking.getText(tweet_id)\n",
    "toks = smoking.getTokens(tweet_id)\n",
    "print([token.text for token in toks])\n",
    "filtered = smoking.getFilteredTokens(tweet_id)\n",
    "print([f.text for f in filtered])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END OF ANSWER cut above here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Revised tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Review of some tweets might lead you to identify text patterns that might not fit with the initial tokenizing or part-of-speech tagging. Fortunately, the spacy tools provide a means for extending the tokenizer for special cases. Here, we review an example of how these tools might be used.\n",
    "\n",
    "Specifically, review of some tweets led to the following concerns: \n",
    "1. The word \"E-cigarette is split by the tokenizer into two separate tokens\n",
    "2. Hashtags are split into the pound symbol (`#`) and the following text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Tokenizing \"E-cigarette\"\n",
    "\n",
    "Consider the following tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoketweet='E-cigarette use by teens linked to later tobacco smoking, study says https://t.co/AhTpFUw0TW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', '-', 'cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that \"E-cigarette\" becomes three tokens. This is not what we want - we want it to be held together as one. \n",
    "To do this, we can add a [special-case tokenizer rule](https://spacy.io/usage/linguistic-features#section-tokenization) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "special_case = [{ORTH: u'e-cigarette', LEMMA: u'e-cigarette', POS: u'NOUN'}]\n",
    "nlp.tokenizer.add_special_case(u'e-cigarette', special_case)\n",
    "nlp.tokenizer.add_special_case(u'E-cigarette', special_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This text says that the text \"e-cigarette\" should be handled by the special case rule saying that it is a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e-cigarette', 'use', 'by', 'teens', 'linked', 'to', 'later', 'tobacco', 'smoking', ',', 'study', 'says', 'https://t.co/AhTpFUw0TW']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(smoketweet)\n",
    "print( [tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we capture \"E-cigarette\" as one token. Note the importance of including both capitalizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Tokenizing hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtags are important in tweets, as we might want to track frequency and trends of mentions. However, the default tokenizer does not capture hashtags as such. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@heal_crypto', ':', '#', 'VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#', 'HealCoin', 'https://t.co/T65Fboq7', 'â€¦']\n"
     ]
    }
   ],
   "source": [
    "hashtag =\"RT @heal_crypto: #VR uses in therapy - for various additictions such as smoking, alcohol, overeating, etc - #HealCoin https://t.co/T65Fboq7â€¦\"\n",
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how \"#VR\" is split into \"#\" and \"VR\". To avoid this, we will can add a specialized [spacy pipeline](https://github.com/explosion/spaCy/issues/503)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_pipe(doc):\n",
    "    merged_hashtag = False\n",
    "    while True:\n",
    "        for token_index,token in enumerate(doc):\n",
    "            if token.text == '#':\n",
    "                if token.nbor() is not None:\n",
    "                    start_index = token.idx\n",
    "                    end_index = start_index + len(token.nbor().text) + 1\n",
    "                    if doc.merge(start_index, end_index) is not None:\n",
    "                        merged_hashtag = True\n",
    "                        break\n",
    "        if not merged_hashtag:\n",
    "            break\n",
    "        merged_hashtag = False\n",
    "    return doc\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "\n",
    "doc = nlp(\"twitter #hashtag\")\n",
    "assert len(doc) == 2\n",
    "assert doc[0].text == 'twitter'\n",
    "assert doc[1].text == '#hashtag'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This routine looks at tokens starting with '#' and adds the \"nbor\" - the next token - to it. This is added to the [spacy pipeline](https://spacy.io/usage/processing-pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returning to our original example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@heal_crypto', ':', '#VR', 'uses', 'in', 'therapy', '-', 'for', 'various', 'additictions', 'such', 'as', 'smoking', ',', 'alcohol', ',', 'overeating', ',', 'etc', '-', '#HealCoin', 'https://t.co/T65Fboq7', 'â€¦']\n"
     ]
    }
   ],
   "source": [
    "parsed=nlp(hashtag)\n",
    "print([tok.text for tok in parsed])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customization of pipelines such as this is often an important part of NLP work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
