{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media and Human-Computer Interaction - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  *Goal*: Learn how to retrieve, manage, and save social media posts.\n",
    "\n",
    "Specifically, we will retrieve, annotate, process, and interpret Twitter data on health-related issues such as depression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "References:\n",
    "* [Mining Twitter Data with Python (Part 1: Collecting data)](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n",
    "* The [Tweepy Python API for Twitter](http://www.tweepy.org/)\n",
    "\n",
    "Required Software\n",
    "* [Python 3](https://www.python.org)\n",
    "* [NumPy](http://www.numpy.org) - for preparing data for plotting\n",
    "* [Matplotlib](https://matplotlib.org) - plots and garphs\n",
    "* [jsonpickle](https://jsonpickle.github.io) for storing tweets. \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import jsonpickle\n",
    "import json\n",
    "import random\n",
    "import time\n",
    "import tweepy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Introduction\n",
    "\n",
    "Analysis of social-media discussions has grown to be an important tool for biomedical informatics researchers, particularly for addressing questions relevant to public perceptions of health and related matters. Studies have examination of a range of topics at the intersection of health and social media, including studies of how [Facebook might be used to commuication health information](http://www.jmir.org/2016/8/e218/) how Tweets might be used to understand how smokers perceive [e-cigarettes, hookahs and other emerging smoking products](https://www.jmir.org/2013/8/e174/), and many others.\n",
    "\n",
    "Although each investigation has unique aspects, studies of social media generally share several common tasks. Data acquisition is often the first challenge: although some data may be freely available, there are often [limits](https://dev.twitter.com/rest/public/rate-limits) as to how much data can be queried easily. Researchers might look out for [opportunities for accessing larger amounts of data](https://www.wired.com/2014/02/twitter-promises-share-secrets-academia/). Some studies contract with [commercial services providing fee-based access](https://gnip.com). \n",
    "\n",
    "Once a data set is hand, the next step is often to identify key terms and phrases relating to the research question. Messages might be annotated to indicate specific categorizations of interest - indicating, for example, if a message referred to a certain aspect of a disease or symptom. Similarly, key words and phrases regularly occurring in the content might also be identified. Natural language and text processing techniques might be used to extract key words, phrases, and relationships, and machine learning tools might be used to build classifiers capable of distinguishing between types of tweets of interest. \n",
    "\n",
    "This module presents a preliminary overview of these techniques, using Python 3 and several auxiliary libraries to explore the application of these techniques to Twitter data. \n",
    "  \n",
    "  1. Configuration of tools to access Twitter data\n",
    "  2. Twitter data retrieval\n",
    "  3. Searching for tweets\n",
    "\n",
    "Our case study will apply these topics to Twitter discussions of smoking and tobacco. Although details of the tools used to access data and the format and content of the data may differ for various services, the strategies and procedures used to analyze the data will generalize to other tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Configuration of tools to access Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Twitter](www.twitter.com) provides limited capabilities for searching tweets through an Application Programming Interface (API) based on Representational State Transfer (REST).  [REST](https://doi.org/10.1145/337180.337228) is an approach to using web-based Hypertext-Transfer Protocol (HTTP) requests as APIs. \n",
    "\n",
    "Essentially, a REST API specifies conventions for HTTP requests that might be used to retrieve specific data items from a remote server. Unlike traditional HTTP requests, which return HTML markup to be rendered in web browsers, REST APIs return data formatted in XML or JSON, suitable for interpretation by computer programs. REST APIs from familiar websites underlie frequently-seen functionality such as embedded twitter widgets and \"like/share\" links, among others.\n",
    "\n",
    "Commercial REST applications often use \"API-Keys\" - unique identifiers used to associate requests with registered accounts. Here, we will walk through the process of registering for Twitter API keys and using a Python library to manage the details of making a Twitter API request and receiving a response.\n",
    "\n",
    "1.1 Registering for a Twitter API key\n",
    "\n",
    "1.1.1 *Signup for Twitter* The first step in registering for a Twitter API key is to [signup](https://twitter.com/signup) for an account. If you dont' want to post anything or to use the account in any way that might be linked to your regular email adddress, you might want to create a special-purpose account using a service such as gmail, and use this new email address for the twitter account.\n",
    "\n",
    "1.1.2. *Create a Twitter application*: Go to  [Twitter's developer site](https://dev.twitter.com) and click on \"My Apps\". Click on \"Create New App\" in the upper right and then fill out the form. The main thing that you need to focus on here is the application name, description, and website. The rest can be ignored.\n",
    "\n",
    "Creating the application will lead to the display of some information with some URLs and a few tabs. Look under \"Keys and Access Tokens\" to see the Consumer API key and API Secret - these will come in handy later.\n",
    "\n",
    "There will also be a button that says \"Create my access token\". Press this button and make a note of the Access Token and Access Token Secret values that are displayed. \n",
    "\n",
    "Although hese tokens are always available on the application page, for the purpose of this exercise, it's best to store them in Python variables directly in this Jupyter notebook. Execute the following insstructions, substituting the keys for your application for the phrases \"YOUR-CONSUMER-KEY\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Note that the following should be redacted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key='D2L4YZ2YrO1PMix7uKUK63b8H'\n",
    "consumer_secret='losRw9T8zb6VT3TEJ9JHmmhAmn1GXKVj30dkiMv9vjhXuiWek9'\n",
    "access_token='15283934-iggs1hiZAPI2o5sfHWMfjumTF7SvytHPjpPRGf3I6'\n",
    "access_secret='bOvqssxS97PGPwXHQZxk83KtAcDyLhRLgdQaokCdVvwFi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, you know have all that you need to start accessing Twitter. Using these keys and the information in the [Twitter Developer Documentation](https://dev.twitter.com/docs), you might conceivably create web requests to search for tweets, post, and read your timeline. In practice, it's a bit more complicated, so most folks use third-party tools that take care of the hard work. \n",
    "\n",
    "1.1.3 *Try the Tweepy library*: [Tweepy](http://www.tweepy.org) is a Python 3 library for using the Twitter API. Like other similar libraries - there are many for Python and other languages - Tweepy takes care of the details of authorization and provides a few simple function calls for accessing the API.  \n",
    "\n",
    "The first step in using Tweepy is *authorization* - establishing your credentials for using the Twitter API. Tweepy uses the [OAuth](http://www.oauth.net) authorization framework, which is widely used for both API and user access to services provided over HTTP. Fortunately Tweepy hides the oauth details. All you need to do is to make a few calls to the Tweepy library and you're all set to go. Run the following code, making sure that the four variables are set to the values you were given when you registered your Twitter application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tweepy.api.API at 0x10c301550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this worked correctly, you should see something like this \n",
    "```\n",
    "<tweepy.api.API at 0x109da36d8>\n",
    "``` \n",
    "\n",
    "If you get an error message, please check your keys and tokens to ensure that they are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Twitter data retrieval\n",
    "\n",
    "Now that you have successfully accessed the Twitter API, it's time to access the data. The simplest thing to do is to grab some Tweets off of your timeline. Try the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_ten = []\n",
    "i =0\n",
    "for tweet in tweepy.Cursor(api.home_timeline,tweet_mode='extended').items(10):\n",
    "    top_ten.append(tweet._json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several key componnents to this block of code:\n",
    "* ```api.home_timeline``` is a component of the API object, referring to the user timeline - the tweets shown on your home page.\n",
    "* ```tweepy.Cursor``` is a construct in the Tweepy API that supports navigation through a large set of results.\n",
    "* ```tweepy.Cursor(api.home_timeline).items(10)``` essentially asks Tweepy to set up a cursor for the home timeline and then to get the first 10 items in that set. The result is a Python Iterator, which can be used to examine the items in the set in turn.\n",
    "* We will grab the JSON representation of each tweet (stored as \"tweet.\\_json\") for maximum flexibility.\n",
    "* The loop takes each of those objects an adds them into a Python array.\n",
    "\n",
    "Now, each of the items in ```top_ten``` is a Tweet object. Let's take a look inside. We'll start by grabbing the first text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet1=top_ten[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and looking at its text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Roseanne review ‚Äì bittersweet reboot explores life under Trump https://t.co/HumUbE3NOl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check for the length of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet1['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `full_text` of the tweet might  contain the length beyond the original 140 characers assoicated with tweets. This value is returned in response to the `tweet_mode='extended'` argument to `tweepy.Cursor()`. Without that argument, a `text` field is returned instead of `full_text`, containing only the first 14 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also examine when the tweet was created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue Mar 27 14:28:51 +0000 2018'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. whether it has been favorited..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['favorited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. The unique ID String of the Tweet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'978639995690012672'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['id_str']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and the name of the Twitter user responsible for the post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Guardian'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['user']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at another tweet in the list.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet1=top_ten[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'978639962533896192'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['id_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"MARCH MAMMAL MADESS #MMM2018 \\nUpdated bracket after Friday's four match-ups! Follow this thread for today's round 2 match-ups:\\n\\nOrinoco Crocodile vs Bothrops asper\\nSecretary Bird vs Mantis Shrimp\\nEurasian Eagle Owl vs Komodo dragon\\nCommon Octopus vs Tardigrade https://t.co/lrOl3TMSkh\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check to see if a tweet is a retweet by seeing if it has the 'retweeted_status' attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'retweeted_status' in tweet1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the tweet is a retweet, the `retweeted_status` field will hold the original tweet - all of the fields contained in the main tweet can be found in the tweet contained in `retweeted_status`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet3=top_ten[3]\n",
    "'retweeted_status' in tweet3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The twitter API supports many other details for users, tweets, and other entities. See [The Twitter API Overview](https://dev.twitter.com/overview/api) for general details and subpages about [Tweets](https://dev.twitter.com/overview/api/tweets), [Users](https://dev.twitter.com/overview/api/users) and related pages for specific details of other data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1: Retweets\n",
    "\n",
    "Here, we're going to look at the contents of a retweet as compared to an original tweet. \n",
    "\n",
    "## 1.1.1 Finding retweets\n",
    "\n",
    "Using the `retweeted_status` field, find a retweet.  You might have to run the tweepy Cursor search above more than once.  \n",
    "\n",
    "## 1.1.2 Examining rewteets\n",
    "\n",
    "Compare the text of the retweet and the original tweet. How do they differ? Which one should we analyze and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*ANSWER FOLLOWS - cut below here*\n",
    "\n",
    "1.1.1 Finding retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2=top_ten[1]\n",
    "'retweeted_status' in t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3=top_ten[2]\n",
    "'retweeted_status' in t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4=top_ten[3]\n",
    "'retweeted_status' in t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 Examining retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @TheJR: Celebrating #WomenHistoryMonth with Marie Tharp: geologist and oceanographic cartographer whose work led to acceptance of the th‚Ä¶\n",
      "Celebrating #WomenHistoryMonth with Marie Tharp: geologist and oceanographic cartographer whose work led to acceptance of the theories of plate tectonics and continental drift. #EXP375 #NSFFunded #IODP https://t.co/MyzmxyiPDt\n"
     ]
    }
   ],
   "source": [
    "print(t4['full_text'])\n",
    "print(t4['retweeted_status']['full_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original tweet contains the full text, but does not contain the `RT` leader. If we're interested in looking at patterns of text use in tweets, we will stick with the original. However, we might, or some other analyses, be interested in retweeting patterns, including who has been retweeted and how frequently. For now, we'll stick with the original. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT*\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Searching for tweets\n",
    "\n",
    "Our next major goal will be to search for Tweets. Effective searching requires both construction of useful queries (the hard part) and use of the Tweepy search API (the easy part).\n",
    "\n",
    "## 1.3.1 Formulating a query\n",
    "\n",
    "Formulating an effective search query is often a challenging, iterative process. Trying some searches in the Twitter web page is a good way to see both how a query might be formulated and which queries might be most useful.\n",
    "\n",
    "If you look carefully at the URL bar in your browser after running a search, you might notice that the search term is embedded in the URL. Thus, if you search for \"depression\", you might see a URL that looks like https://twitter.com/search?q=depression. You might also see \"&src=typed\" at the end of the URL, indicating that the search was typed by hand.\n",
    "\n",
    "You can also use Tweepy to conduct a search, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tlist = api.search(\"smoking\",lang=\"en\",count=10,tweet_mode='extended')\n",
    "tweets = [t._json for t in tlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This search will find the first 10 English tweets matching the term \"depression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ObreezySa @super_pacman94 @Nhlanhl11835774 @akaworldwide Mr lady what are you smoking?'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[0]['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the text for these tweets. This is a good way to check to ensure that we're getting what we think we should be getting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [c['full_text'] for c in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@ObreezySa @super_pacman94 @Nhlanhl11835774 @akaworldwide Mr lady what are you smoking?',\n",
       " '@BestNoobEU @wyzetoothpick @Lazerchickenzzz @RiotQuickshot lack of fans? g2 has the largest fanbase after fnatic what are you smoking, also if you think they werent/arent kings of europe rewatch these games, they rekked everyone easily',\n",
       " 'RT @alfrdhy: I really want to quit smoking by end of this year I regret picking up smoking againnnnnn helpppp mi üò≠üò≠üò≠',\n",
       " 'I swear every time Marie gets on I think she is smoking something.  She is so out there.  Wow https://t.co/KsopuZN1hb',\n",
       " 'RT @DinhoLdn: Young Thug really said \"My weed loud you smoking libraries\" creativity knows no bounds',\n",
       " 'really trynna stop smoking but all this anger building up and i don‚Äôt know where to put it‚òπÔ∏è',\n",
       " 'RT @DinhoLdn: Young Thug really said \"My weed loud you smoking libraries\" creativity knows no bounds',\n",
       " 'RT @DinhoLdn: Young Thug really said \"My weed loud you smoking libraries\" creativity knows no bounds',\n",
       " 'RT @M_blessed_soul: #‡§∏‡•ç‡§µ‡§∏‡•ç‡§•_‡§ú‡•Ä‡§µ‡§®\\n#SaintMSG_Initiative54\\nMillions of thnx to @Gurmeetramrahim Ji, for inspiring pepl to quit bad habits lyk‚Ä¶',\n",
       " 'RT @Its_Falmata: Who has seen that video of a young almajiri boy not more than 5 years of age smoking!!!! We northerners have failed this y‚Ä¶']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You may see some tweets that don't match exactly - perhaps using 'smoke' instead of 'smokiing'. This suggests that Twitter uses <em>stemming</em> - removing suffixes and variations to get to the core of the word - to increase search accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should be able to evaluate the results to see if we are on the right track. If we aren't, we'd want to try some different queries. For now, it looks good, so let's move on.\n",
    "\n",
    "## 1.3.2 Collecting and characterizing a larger corpus\n",
    "\n",
    "Our original query only retrieved 10 tweets. This is a good start, but probably not enough for anything serious. We can loop through several times to create a longer list, with a delay between searches to avoid overstaying our welcome with Twitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):\n",
    "    new_tweets = api.search(\"smoking\",lang=\"en\",tweet_mode='extended',count=100)\n",
    "    nt = [t._json for t in new_tweets]\n",
    "    tweets= tweets+nt\n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point, we might want to know something about the tweets that we have retrieved. As our goal is to shoot for linguistic diversity, we want to make sure that we don't have too many retweets, that we have a wide range of authors, and that we have enough different tweets (not too many repeats).  Let's run through the tweets and count the number of authors, the number of  retweets, and the number of times each tweet is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAuthors(tweets):\n",
    "    authors={}\n",
    "    retweets=0\n",
    "    uniqTweets={}\n",
    "    for t in tweets:\n",
    "        # is it a retweet? If so, increment\n",
    "        if 'retweeted_status' in t:\n",
    "            retweets = retweets+1\n",
    "        # get tweet author name\n",
    "        uname = t['user']['name']\n",
    "        # if not in authors, put it in with zero articles\n",
    "        if uname not in authors:\n",
    "            authors[uname]=0\n",
    "        authors[uname]=authors[uname]+1\n",
    "        id=t['id_str']\n",
    "        if id not in uniqTweets.keys():\n",
    "            uniqTweets[id]=0\n",
    "        uniqTweets[id]=uniqTweets[id]+1\n",
    "\n",
    "    # sort uniq tweets\n",
    "    uts=[]\n",
    "    for t,entry in uniqTweets.items():\n",
    "        uts.append((t,entry))  \n",
    "        uts.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    return (retweets,authors,uts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(retweets,authors,uniq) = getAuthors(tweets)\n",
    "retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(authors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might see a lot of retweets here - I saw at least 80% in one instance, with about 193 authors. This suggests that this corpus has a good many authors with multiple tweets. \n",
    "\n",
    "To explore this, let's look at the histogram of the number of tweets/author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the distribution of authors, we can use the [NumPy](http://www.numpy.org) and [Matplotlib](http://matplotlib.org) libraries to extract the number of tweets from each user (given by authors.values()) and to plot a histogram..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAD/1JREFUeJzt3W2MXGd5xvH/5dgBTIrlttjb1pAQqgQLiQBCQJtWnSpN\nS0GK/aUp9M0Opf1QUKK2QnFSVVkJCYVICEWiXxAQbRFJm6QgOxXUxnVGFbS8xilREraoQAgULwIS\nUJQqvPjuhzmJjLvrndmdl/Xj/09a5ZyTc85zH611zXPumTObqkKSdPbbNOsCJEnjYaBLUiMMdElq\nhIEuSY0w0CWpEQa6JDVi1UBPckmS40nu6/77/STXJtme5EiSxSSHk2ybRsGSpOVllM+hJ9kEfAN4\nDfA24LtVdUuS64HtVXVgMmVKklYzasvlt4D/rqpHgT3AQrd9Adg7zsIkSaMZNdB/H7i9W95ZVUsA\nVXUC2DHOwiRJoxk60JNsAa4C7uo2nd6r8TsEJGmGNo+w7+8CX6iq73TrS0l2VtVSkjng28sdlMSg\nl6Q1qKqMsv8oLZc3AXecsn4I2N8t7wMOnqGoZn9uuummmdfg9XltXl97P2sxVKAn2crgDdGPnLL5\nXcCVSRaBK4Cb11SBJGkshmq5VNWTwPNP2/Y9BiEvSdoAfFJ0nXq93qxLmKiWr6/lawOv71w00oNF\naxogqUmPIUmtSUJN8E1RSdIGZqBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGg\nS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5ppubmLiLJ1H7m\n5i6a9SVPjH8kWtJMJQGmmRHhbMikif2R6CTbktyV5OEkDyZ5TZLtSY4kWUxyOMm2tZUtSRqHYVsu\ntwIfq6rdwGXAl4ADwNGquhQ4BtwwmRIlScNYteWS5HnA8ap68WnbvwT8RlUtJZkD+lX1kmWOt+Ui\naUW2XJY3qZbLi4DvJLktyX1J3pdkK7CzqpYAquoEsGP0kiVJ47J5yH1eCby1qj6f5D0M2i2nv8St\n+JI3Pz//zHKv16PX641cqCS1rN/v0+/313WOYVouO4H/qKqLu/VfYxDoLwZ6p7Rc7u167Kcfb8tF\n0opsuSxvIi2Xrq3yaJJLuk1XAA8Ch4D93bZ9wMFRBpYkjddQn0NPchnwfmAL8BXgGuA84E7gBcAj\nwNVV9fgyxzpDl7QiZ+jLW8sM3QeLJM2Ugb68iT1YJEna+Ax0SWqEgS5JjTDQJakRBrokNcJAl6RG\nGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSB\nLkmNMNAlqREGuiQ1wkCXpEYY6JLUiM3D7JTka8D3gZPAj6rq1Um2A/8IXAh8Dbi6qr4/oTolSasY\ndoZ+EuhV1Suq6tXdtgPA0aq6FDgG3DCJAiVJwxk20LPMvnuAhW55Adg7rqIkSaMbNtAL+ESSzyV5\nS7dtZ1UtAVTVCWDHJAqUJA1nqB46cHlVfSvJ84EjSRYZhPypTl9/xvz8/DPLvV6PXq83YpmS1LZ+\nv0+/31/XOVK1Yg4vf0ByE/AE8BYGffWlJHPAvVW1e5n9a9QxJJ07knCG+eAkRuRsyKQkVFVGOWbV\nlkuSrUku6JafC/w28ABwCNjf7bYPODhStZKksVp1hp7kRcBHGbyEbgY+XFU3J/lZ4E7gBcAjDD62\n+PgyxztDl7QiZ+jLW8sMfeSWy6gMdElnYqAvbyItF0nS2cFAl6RGGOiS1AgDXZIaYaBLUiMMdElq\nhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY\n6JLUCANdkhphoEtSIwx0SWqEgS5JjRg60JNsSnJfkkPd+vYkR5IsJjmcZNvkypQkrWaUGfp1wEOn\nrB8AjlbVpcAx4IZxFiZJGs1QgZ5kF/B64P2nbN4DLHTLC8De8ZYmSRrFsDP09wBvB+qUbTuragmg\nqk4AO8ZcmyRpBJtX2yHJG4Clqro/Se8Mu9ZK/2N+fv6Z5V6vR693ptNI0rmn3+/T7/fXdY5UrZjD\ngx2SdwJ/BPwYeA7wM8BHgVcBvapaSjIH3FtVu5c5vlYbQ9K5KwlnmA9OYkTOhkxKQlVllGNWbblU\n1Y1V9cKquhh4I3Csqv4YuAfY3+22Dzg4Yr2SpDFaz+fQbwauTLIIXNGtS5JmZNWWy7oHsOUi6Qxs\nuSxvIi0XSdLZwUCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgD\nXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNWDXQ\nkzwryWeSHE/yYJJ3dtu3JzmSZDHJ4STbJl+uJGklqarVd0q2VtWTSc4DPgX8NXAV8N2quiXJ9cD2\nqjqwzLE1zBiSzk1JgGlmRDgbMikJVZVRjhmq5VJVT3aLz+qOeQzYAyx02xeAvaMMLEkar6ECPcmm\nJMeBE0C/qh4CdlbVEkBVnQB2TK5MSdJqNg+zU1WdBF6R5HnA4SQ9/v890or3MPPz888s93o9er3e\nqHVKUtP6/T79fn9d5xiqh/5TByR/C/wv8KdAr6qWkswB91bV7mX2t4cuaUX20Jc3kR56kp9/+hMs\nSZ4DXAkcBw4B+7vd9gEHR6pWkjRWw7RcfgFYyOBldBPwoar6166nfmeSNwOPAFdPsE5J0ipGbrmM\nPIAtF0lnYMtleRP72KIkaeMz0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAl\nqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIa\nYaBLUiNWDfQku5IcS/JgkgeSXNtt357kSJLFJIeTbJt8uZKklaSqzrxDMgfMVdX9SS4AvgDsAa4B\nvltVtyS5HtheVQeWOb5WG0PSuSsJMM2MCGdDJiWhqjLKMavO0KvqRFXd3y0/ATwM7GIQ6gvdbgvA\n3tHKlSSN00g99CQXAS8HPg3srKolGIQ+sGPcxUmShrd52B27dsvdwHVV9USS0+9ZVryHmZ+ff2a5\n1+vR6/VGq1KSGtfv9+n3++s6x6o9dIAkm4F/Bj5eVbd22x4GelW11PXZ762q3cscaw9d0orsoS9v\nIj30zgeBh54O884hYH+3vA84OMrAkqTxGuZTLpcD/wY8wOBltIAbgc8CdwIvAB4Brq6qx5c53hm6\npBU5Q1/eWmboQ7Vc1sNAl3QmBvryJtlykSRtcAa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSB\nLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS\n1AgDXZIaYaBLUiMMdElqxKqBnuQDSZaSfPGUbduTHEmymORwkm2TLVOStJphZui3Ab9z2rYDwNGq\nuhQ4Btww7sIkSaNZNdCr6pPAY6dt3gMsdMsLwN4x1yVJGtFae+g7qmoJoKpOADvGV5IkaS02j+k8\ndab/OT8//8xyr9ej1+uNaVhJGtWzSDK10XbuvJATJ7626n79fp9+v7+usVJ1xiwe7JRcCNxTVS/r\n1h8GelW1lGQOuLeqdq9wbA0zhqRz0yBcp5kR0x9vLRmYhKoa6ZVn2JZLup+nHQL2d8v7gIOjDCpJ\nGr9hPrZ4O/DvwCVJvp7kGuBm4Moki8AV3bo2sLm5i0gytZ+5uYu8NmnKhmq5rGsAWy4bwixua6f1\ne2/52s4FtlxWOGqCLRdJ0gZnoHe8bddG5b9NDcuWS6f12/aWr6/lawOvbwIjTn08Wy6SpJGM68Ei\njWy6DzdonPzdaWMy0GfmKaZ/m6nx8HenjcmWiyQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5J\njdiQn0NfXFzknnvumdp4W7ZsmdpYkjQpGzLQ3/GOd3PHHV9l06bLpjLeeed9ZCrjSGcHn4Q9W23I\nQAc4efL3OHnyz6cy1tatizz11FenMpa08fkk7NnKHrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElq\nxLoCPcnrknwpyX8luX5cRUmSRrfmQE+yCXgv8DvAS4E3JXnJuAo7e/RnXcCE9WddwAT1Z13AhPVn\nXcCE9WddwIaznhn6q4EvV9UjVfUj4B+APeMp62zSn3UBE9afdQET1J91ARPWn3UBE9afdQEbznoC\n/ZeAR09Z/0a3TZI0Axvy0f/zz9/Cs5/9Xs4/fzpf0PXDH352KuNI0iSlam3f2ZDktcB8Vb2uWz8A\nVFW967T9pvmlEJLUjKoa6Ytu1hPo5wGLwBXAt4DPAm+qqofXdEJJ0rqsueVSVT9J8jbgCINe/AcM\nc0manTXP0CVJG8vEnhRt+aGjJLuSHEvyYJIHklw765omIcmmJPclOTTrWsYtybYkdyV5uPs9vmbW\nNY1Tkhu66/pikg8nOX/WNa1Hkg8kWUryxVO2bU9yJMliksNJts2yxrVa4dpu6f5t3p/kn5I8b5hz\nTSTQz4GHjn4M/FVVvRT4FeCtjV3f064DHpp1ERNyK/CxqtoNXAY00y5MciHwZ8ArquplDFqrb5xt\nVet2G4M8OdUB4GhVXQocA26YelXjsdy1HQFeWlUvB77MkNc2qRl60w8dVdWJqrq/W36CQRg09Rn8\nJLuA1wPvn3Ut49bNdn69qm4DqKofV9UPZlzWOP0A+CHw3CSbga3A/8y2pPWpqk8Cj522eQ+w0C0v\nAHunWtSYLHdtVXW0qk52q58Gdg1zrkkF+jnz0FGSi4CXA5+ZbSVj9x7g7Uz3b5FNy4uA7yS5rWsp\nvS/Jc2Zd1LhU1WPAu4GvA98EHq+qo7OtaiJ2VNUSDCZZwI4Z1zMpbwY+PsyOftviOiS5ALgbuK6b\nqTchyRuApe4uJLT3Rx83A68E/q6qXgk8yeD2vQlJLgb+ErgQ+EXggiR/MNuqpqK5yUeSvwF+VFW3\nD7P/pAL9m8ALT1nf1W1rRncrezfwoao6OOt6xuxy4KokXwHuAH4zyd/PuKZx+gbwaFV9vlu/m0HA\nt+JVwKeq6ntV9RPgI8CvzrimSVhKshMgyRzw7RnXM1ZJ9jNoew79YjypQP8c8MtJLuzeXX8j0Non\nJT4IPFRVt866kHGrqhur6oVVdTGD392xqvqTWdc1Lt1t+qNJLuk2XUFbb/4uAq9N8uwkYXB9Lbzp\ne/rd4iFgf7e8DzibJ1Y/dW1JXseg5XlVVT017Ekm8l0urT90lORy4A+BB5IcZ3Crd2NV/ctsK9MI\nrgU+nGQL8BXgmhnXMzZV9Z/dHdUXgJ8Ax4H3zbaq9UlyO9ADfi7J14GbgJuBu5K8GXgEuHp2Fa7d\nCtd2I3A+8InBazKfrqq/WPVcPlgkSW3wTVFJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhph\noEtSI/4PFYWetuDDtQ4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ce28160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = np.array(list(authors.values()))\n",
    "#plt.xticks(range(min(vals),max(vals)+1))\n",
    "plt.hist(vals,np.arange(min(vals)-0.5,max(vals)+1.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at how frequently each tweet is seen we can look at the first few elements in the third item returned by `getAuthors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('978640027659001857', 11),\n",
       " ('978640043882483712', 11),\n",
       " ('978640043890827264', 11),\n",
       " ('978640021979783168', 11),\n",
       " ('978640019572379652', 11),\n",
       " ('978640025494671360', 11),\n",
       " ('978640034428571649', 11),\n",
       " ('978640037138006018', 11),\n",
       " ('978640020276785152', 11)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a broad range of the number of tweets/user, up until roughly tweets, with many users having 10 tweets. This is an intersting pattern, with no immediately obvious interpretation. Understanding the usage patterns might be an intersting area for further work, although larger data sets might be necessary to see meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4. A class for storing Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the complexity of the data being discussed, we should create a class to store our tweets. \n",
    "\n",
    "This class will be based on a dictionary called `tweets`. This dictionary will be indexed by the ID string of the tweet. Each element of the dictionary will itself be a dictionary, with the following contents:\n",
    "* `tweet` will refer to the full tweet\n",
    "* `count` contains the number of times it occurs in the dataset. Generally, this will be zero, but we might have a need for allowing for a tweet to occur multiple times.\n",
    "* `searchTime` will contain the timestamp of when a tweet was searched or added to the set.\n",
    "* `seachTerm` will contain the term used to search for the tweet.\n",
    "\n",
    "We will have methods for adding tweets, retrieving tweets by id, tracking the number of times we see each tweet, and other useful features. We'll also add a routine to find the most frequently-seen tweets by ID.\n",
    "\n",
    "We also provide a `searchTweets` routine to search for a set of unique tweets of a given size matching a given query. This will be called by the default constructor if search terms and counts are provided. \n",
    "\n",
    "Finally, `getText` will retrieve the text that we're interested in for a given tweet. If the tweet is not a retwet, the `full_text` field will be returned. However, if it is a retweet, the `full_text` of the original tweet will be returne. Thus, we will ahve the maximal amount of text available to analyze. \n",
    "\n",
    "This structure will evolve as we go along.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tweets:\n",
    "    \n",
    "    \n",
    "    def __init__(self,term=\"\",corpus_size=100):\n",
    "        self.tweets={}\n",
    "        if term !=\"\":\n",
    "            self.searchTwitter(term,corpus_size)\n",
    "                \n",
    "    def searchTwitter(self,term,corpus_size):\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",tweet_mode='extended',count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                nt = nt_json._json\n",
    "                if self.getTweet(nt['id_str']) is None and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "            time.sleep(5)\n",
    "                \n",
    "    def addTweet(self,tweet,searchTime,term=\"\",count=0):\n",
    "        id = tweet['id_str']\n",
    "        if id not in self.tweets.keys():\n",
    "            self.tweets[id]={}\n",
    "            self.tweets[id]['tweet']=tweet\n",
    "            self.tweets[id]['count']=0\n",
    "            self.tweets[id]['searchTime']=searchTime\n",
    "            self.tweets[id]['searchTerm']=term\n",
    "        self.tweets[id]['count'] = self.tweets[id]['count'] +1\n",
    "        \n",
    "    def getTweet(self,id):\n",
    "        if id in self.tweets:\n",
    "            return self.tweets[id]['tweet']\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def getTweetCount(self,id):\n",
    "        return self.tweets[id]['count']\n",
    "    \n",
    "    def countTweets(self):\n",
    "        return len(self.tweets)\n",
    "    \n",
    "    # return a sorted list of tupes of the form (id,count), with the occurrence counts sorted in decreasing order\n",
    "    def mostFrequent(self):\n",
    "        ps = []\n",
    "        for t,entry in self.tweets.items():\n",
    "            count = entry['count']\n",
    "            ps.append((t,count))  \n",
    "        ps.sort(key=lambda x: x[1],reverse=True)\n",
    "        return ps\n",
    "    \n",
    "    # reeturns tweet IDs as a set\n",
    "    def getIds(self):\n",
    "        return set(self.tweets.keys())\n",
    "    \n",
    "    # save the tweets to a file\n",
    "    def saveTweets(self,filename):\n",
    "        json_data =jsonpickle.encode(self.tweets)\n",
    "        with open(filename,'w') as f:\n",
    "            json.dump(json_data,f)\n",
    "    \n",
    "    # read the tweets from a file \n",
    "    def readTweets(self,filename):\n",
    "        with open(filename,'r') as f:\n",
    "            json_data = json.load(f)\n",
    "            incontents = jsonpickle.decode(json_data)   \n",
    "            self.tweets=incontents\n",
    "        \n",
    "    def getSearchTerm(self,id):\n",
    "        return self.tweets[id]['searchTerm']\n",
    "    \n",
    "    def getSearchTime(self,id):\n",
    "        return self.tweets[id]['searchTime']\n",
    "    \n",
    "    def getText(self,id):\n",
    "        tweet = self.getTweet(id)\n",
    "        text=tweet['full_text']\n",
    "        if 'retweeted_status'in tweet:\n",
    "            original = tweet['retweeted_status']\n",
    "            text=original['full_text']\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets2 = Tweets(\"smoking\",100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've got a good solid set of tweets to work with. Note that we also route some routines above to save and load tweets from a file. Let's try them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets2.saveTweets('tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's  do some quick checks to confirm that we've got the right data out. Note that in future runs, you can just start here to read in your tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets3=Tweets()\n",
    "tweets3.readTweets('tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets3.countTweets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[According to Python documentation](https://docs.python.org/2/reference/expressions.html#id24) dictionaries are equal if the keys and values are equal, so this looks good. To check in more detail, we can look at the keys, using subtraction to indicate set difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets2.getIds()-tweets3.getIds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets3.getIds()-tweets2.getIds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok, so we've got the same set of tweets IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my dad literally made a deal that if i get 1,000 retweets he‚Äôll stop smoking. please get this to 1,000. like this isn‚Äôt a joke\n",
      "my dad literally made a deal that if i get 1,000 retweets he‚Äôll stop smoking. please get this to 1,000. like this isn‚Äôt a joke\n"
     ]
    }
   ],
   "source": [
    "tweet_id=random.choice(list(tweets3.getIds()))\n",
    "t2=tweets2.getText(tweet_id)\n",
    "t3=tweets3.getText(tweet_id)\n",
    "print(tweets2.getText(tweet_id))\n",
    "print(tweets3.getText(tweet_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t3==t2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot checks like this give some confidence that the loaded tweets are identical to the saved tweets. We might also run a slightly more rigorous check by iterating through the list to look for similarities. Since we know that the two dictionaries have identical sets of keys, we can iterate through the keys of one to get entries and compare equalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "errs =[]\n",
    "for id in tweets2.getIds():\n",
    "    t2 =tweets2.getTweet(id)\n",
    "    t3 =tweets3.getTweet(id)\n",
    "    if t2 != t3:\n",
    "        errs.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. No errors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Exercise 1.2: Dataset diversity\n",
    "\n",
    "Having collected a data set, we might want to characterize it in different ways. We saw above how to identify the number of authors represented in a set of tweets. Here, we examine a slightly different question - what is the elapsed time period covered by a set of tweets? In other words, what are the  times of the first and last tweets in the set?\n",
    "\n",
    "To do this, we'll need some help from Python libraries. Before we get into that, let's get a couple of tweets from our set. \n",
    "\n",
    "## 1.2.1 Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tid1=random.choice(list(tweets3.getIds()))\n",
    "tid2=random.choice(list(tweets3.getIds()))\n",
    "tweet1=tweets3.getTweet(tid1)\n",
    "tweet2=tweets3.getTweet(tid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now look at their creation times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue Mar 27 14:34:19 +0000 2018'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet1['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tue Mar 27 14:32:58 +0000 2018'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet2['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the python [datetime](https://docs.python.org/2/library/datetime.html) library, and the *strptime* function in particular to convert these strings to datetime objects capable of being compared and manipulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do this, we call *strptime* with a string pattern matchings of the strings returned in the tweet object. Specifically, we can see that each timestamp has a 3 letter string indicating a day of the week, the month, the date, the time in hh:mm:ss format, a time-zone indicate (\"+000\") and the year. These items can be specified in a string argument as \"%a\" ,\" \"%b\", \"%d\", \"%H\", \"%M\", \"%S\", \"%z\" and \"%Y\", respectively, thus providing a pattern to be used to create the time object, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = datetime.strptime(tweet1['created_at'], \"%a %b %d %H:%M:%S %z %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 3, 27, 14, 34, 19, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t2 = datetime.strptime(tweet2['created_at'], \"%a %b %d %H:%M:%S %z %Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "we can then find the difference between the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(-1, 86319)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2-t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take these *datetime* objects and convert them into dates, which then might be compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.date() == t1.date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.2. Min and Max\n",
    "\n",
    "Find the times of the minimum (earliest) and maximum (latest) tweets in the collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-27 14:31:17+00:00\n",
      "2018-03-27 14:35:11+00:00\n"
     ]
    }
   ],
   "source": [
    "def getExtremes(tweets):\n",
    "    times = []\n",
    "    for id in tweets.getIds():\n",
    "        ctimestring = tweets.getTweet(id)['created_at']\n",
    "        ctime = datetime.strptime(ctimestring,\"%a %b %d %H:%M:%S %z %Y\")\n",
    "        times.append(ctime)\n",
    "    mint = min(times) \n",
    "    maxt = max(times)\n",
    "    return (mint,maxt)    \n",
    "    \n",
    "(mint,maxt) = getExtremes(tweets3)\n",
    "print(mint)\n",
    "print(maxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT*\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Frequency\n",
    "\n",
    "Write a routine to find the distribution of the teets in the set by date. You might do this by creating a dictionary that has the dates of tweets as keys.  This routine will be very similar to the routine written above to count the number of tweets by author.\n",
    "\n",
    "Note that for this dataset you will probably have all tweets coming from the same date. However, your routine should be generally enough to find the number of tweets for any date represented in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDateFrequency(tweets):\n",
    "    dates={}\n",
    "    for id in tweets.getIds():\n",
    "        ctimestring = tweets.getTweet(id)['created_at']\n",
    "        ctime = datetime.strptime(ctimestring,\"%a %b %d %H:%M:%S %z %Y\")\n",
    "        cdate = ctime.date()\n",
    "        if cdate not in dates:\n",
    "            dates[cdate]=0\n",
    "        dates[cdate]=dates[cdate]+1\n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreq = getDateFrequency(tweets3)\n",
    "len(dfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys=list(dfreq.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2018, 3, 27)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1=keys[0]\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreq[d1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.3 Other forms of data diversity\n",
    "\n",
    "The open-ended nature of social media makes true sampling almost impossible. Unlike sampling based on geographical constraints such as place of residence, data sampled from social media does not draw from any well-characterized population. More simply stated, we might know how many people live in a city or town, but we don't know how many people might have tweeted on a given topic at any given time. \n",
    "\n",
    "However, we can look at the data to ensure that has some diversity. Our exploration of authors provides one example:\n",
    "a data set with a range of authors may cover more topics than one with a much smaller number of authors. Diversity of dates and times might help in the same way.  \n",
    "                                                                                                                                                                                    The following questions will encourage you to think about other forms of data diversity. \n",
    "                                                                                                                                                                                    \n",
    "## 1.3.1  Why might diversity  of times be of interest?                                                                                                                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*\n",
    "\n",
    "Tweets might contain different content at different times of days, different days of the week, of even based on season.  For example, discussions of recreational drug use might be more common on evenings and weekends, just as tweets about skiing and snowstorms might be more likely in winter.  \n",
    "\n",
    "*END CUT*\n",
    "\n",
    "## 1.3.2  How might you ensure a diversity of times?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*\n",
    "\n",
    "Collecting a much larger corpus over weeks or months will cover a range of times. If longitudinal collection of tweets over many months is prohibitively difficult, you might try collecting snapshots - perhaps 100 or 1000 tweets/day over a course of time.  \n",
    "\n",
    "*END CUT*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.3 What other forms of data diversity might be of interest, and how might they be achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*ANSWER FOLLOWS - cut below here*\n",
    "\n",
    "Geographic diversity is important if you want to exmaine differences in patterns across different regions or countries. \n",
    "This [stack overflow post](https://stackoverflow.com/questions/20169467/how-to-convert-from-longitude-and-latitude-to-country-or-city) discusses how you might convert longitude/latitude information available in tweets into states and regions.\n",
    "\n",
    "Other cases might require tweets from a range of languages. The 'lang' attribute of tweet indicates the language in which it was written.\n",
    "\n",
    "In either case, you might proceed as we did with unique tweet IDs. Simply continue trying to search until you get a braod enough range of values for your questions of interest.\n",
    "\n",
    "*END CUT*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.4 Increasing Diversity in Datasets\n",
    "\n",
    "The `searchTweets` routine defined in the `Tweets` class atempts to find diverse tweets by ensuring that no tweet is included twice. However, we might see multiple tweets retweeting the same original. How might you modify `searchTweets` to avoid this sort of duplication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "*ANSWER FOLLOWS - cut below here*\n",
    "\n",
    "Basically, you'd have to revise the search to keep a list of tweet ids that have been seen - either as the main id, or as the id of the tweet that has been retweeted, and check that list before adding a new teweet. \n",
    "the revised routine would look something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def searchTwitter(self,term,corpus_size):\n",
    "        seen={}\n",
    "        searchTime=datetime.now()\n",
    "        while (self.countTweets() < corpus_size):\n",
    "            new_tweets = api.search(term,lang=\"en\",tweet_mode='extended',count=10)\n",
    "            for nt_json in new_tweets:\n",
    "                origid = None\n",
    "                nt = nt_json._json\n",
    "                id=nt['id_str']\n",
    "                if 'retweeted_status' in nt:\n",
    "                    origid=nt['retweeted_status']['id_str']\n",
    "                if nt not in seen and origid not in seen and self.countTweets() < corpus_size:\n",
    "                    self.addTweet(nt,searchTime,term)\n",
    "                    seen[id]=1\n",
    "                    if origid is not None:\n",
    "                        seen[origid]=1\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*END CUT*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Some final notes\n",
    "Note that we might find that we will want to add additional fields to this file. We can always rewreite the file as needed. Saving the file as is gives us a good record that we can work from, without having to recreate the dataset. \n",
    "\n",
    "Now that you've mastered the basics of retrieving Twitter data, you can move on to [Part 2](SocialMedia - Part 2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
